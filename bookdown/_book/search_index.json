[
["index.html", "Notas de clase del curso de introducción a Data Science Capítulo-1 Introducción", " Notas de clase del curso de introducción a Data Science Diego Kozlowski y Natsumi Shokida 2019-08-30 Capítulo-1 Introducción "],
["presentacion.html", "1.1 Presentación", " 1.1 Presentación En los últimos años se han difundido muchas herramientas estadísticas novedosas para el análisis de información socioeconómica y geográfica. En particular el software denominado “R”, por tratarse de un software libre, se extiende cada vez más en diferentes disciplinas y recibe el aporte de investigadores e investigadoras en todo el mundo, multiplicando sistemáticamente sus capacidades. Este programa se destaca, entre otras cosas, por su capacidad de trabajar con grandes volúmenes de información, utilizar múltiples bases de datos en simultáneo, generar reportes, realizar gráficos a nivel de publicación y por su comunidad de usuarios que publican sus sintaxis y comparten sus problemas, hecho que potencia la capacidad de consulta y de crecimiento. A su vez, la expresividad del lenguaje permite diseñar funciones específicas que permiten optimizar de forma personalizada el trabajo cotidiano con R. "],
["objetivos-del-curso.html", "1.2 Objetivos del curso", " 1.2 Objetivos del curso El presente Taller tiene como objetivo principal introducir a los participantes en la ciencia de datos, sobre la base de la utilización del lenguaje R aplicado procesamiento de diferentes bases de datos provistas por el programa de Gobierno Abierto y la Encuesta Permanente de Hogares (EPH) - INDEC. Se apunta a brindar las herramientas necesarias para la gestión de la información, presentación de resultados y algunas técnicas de modelado de datos, de forma tal que los participantes puedan luego avanzar por su cuenta a técnicas más avanzadas. "],
["temario.html", "1.3 Temario", " 1.3 Temario 1.3.1 clase 1: Introducción al entorno R: Descripción del programa “R”. Lógica sintáctica del lenguaje y comandos básicos Presentación de la plataforma RStudio para trabajar en “R” Caracteres especiales en “R” Operadores lógicos y aritméticos Definición de Objetos: Valores, Vectores y DataFrames Tipos de variable (numérica, de caracteres, lógicas) Lectura y Escritura de Archivos 1.3.2 clase 2: Tidyverse: Limpieza de Base de datos: Renombrar y recodificar variables, tratamiento de valores faltantes (missing values/ NA´s) Seleccionar variables, ordenar y agrupar la base de datos para realizar cálculos Creación de nuevas variables Aplicar filtros sobre la base de datos Construir medidas de resumen de la información Tratamiento de variables numéricas (edad, ingresos, horas de trabajo, cantidad de hijos / componentes del hogar, entre otras). 1.3.3 clase 3: Estadística descriptiva Introducción a probabilidad Introducción a distribuciones El problema de la inversión Estadística Población y muestra Estimadores puntuales, tests de hipótesis Boxplots, histogramas y kernels 1.3.4 clase 4: Visualización de la información Gráficos básicos de R (función “plot”): Comandos para la visualización ágil de la información Gráficos elaborados en R (función “ggplot”): Gráficos de línea, barras, Boxplots y distribuciones de densidad Parámetros de los gráficos: Leyendas, ejes, títulos, notas, colores Gráficos con múltiples cruces de variables. 1.3.5 clase 5: Documentación en R Manejo de las extensiones del software “Rmarkdown” y “RNotebook” para elaborar documentos de trabajo, presentaciones interactivas e informes: Opciones para mostrar u ocultar código en los reportes Definición de tamaño, títulos y formato con el cual se despliegan los gráficos y tablas en el informe Caracteres especiales para incluir múltiples recursos en el texto del informe: Links a páginas web, notas al pie, enumeraciones, cambios en el formato de letra (tamaño, negrita, cursiva) Código embebido en el texto para automatización de reportes 1.3.6 clase 6: Análisis de encuestas Introducción al diseño de encuestas Presentación de la Encuesta Permanente de Hogares Generación de estadísticos de resumen en muestras estratificadas Utilización de los ponderadores 1.3.7 clase 7: Programación funcional Estructuras de código condicionales Loops Creación de funciones a medida del usuario Librería purrr para programación funcional 1.3.8 clase 8: Mapas Utilización de información geográfica en R Elaboración de mapas gestión de shapefiles 1.3.9 clase 9: Shiny Shiny como reportes dinámicos Su utilidad para el análisis exploratorio Lógica de servidor- interfaz de usuario Extensiones del mundo shiny Publicación de resultados 1.3.10 clase 10: Correlación y Modelo Lineal Análisis de correlación. Presentación conceptual del modelo lineal El modelo lineal desde una perspectiva computacional Supuestos del modelo lineal Modelo lineal en R Modelo lineal en el tidyverse 1.3.11 clase 11: Text Mining Introducción al análisis de textos Limpieza Preprocesamiento BoW Stopwords TF-IDF Wordcloud Escrapeo de Twitter "],
["bibliografia-de-consulta.html", "1.4 Bibliografía de consulta", " 1.4 Bibliografía de consulta GWickham, H., &amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. &quot; O’Reilly Media, Inc.&quot;. https://es.r4ds.hadley.nz/ James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An introduction to statistical learning. New York: springer. http://faculty.marshall.usc.edu/gareth-james/ISL/ Wickham, Hadley. ggplot2: elegant graphics for data analysis. Springer, 2016. https://ggplot2-book.org/ 1.4.1 Librerias a instalar install.packages(c(&quot;tidyverse&quot;,&quot;openxlsx&quot;,&quot;xlsx&quot;,&#39;ggplot2&#39;,&#39;GGally&#39;,&#39;ggridges&#39;,&#39;treemapify&#39;,&#39;esquisse&#39;,&#39;cowplot&#39;,&#39;ggthemes&#39;, &#39;ggrepel&#39;,&#39;ggalt&#39;,&#39;kableExtra&#39;,&#39;fs&#39;,&#39;purrr&#39;,&#39;rmarkdown&#39;)) "],
["introduccion-a-r.html", "Capítulo-2 Introducción a R", " Capítulo-2 Introducción a R En esta primera clase revisaremos los fundamentos de R base y el entorno de RStudio. El objetivo es poder comenzar a utilizar el programa, abrir archivos y empezar a experimentar para ganar confianza. Descripción del programa R. Lógica sintáctica del lenguaje y comandos básicos Presentación de la plataforma RStudio para trabajar en R Caracteres especiales en R Operadores lógicos y aritméticos Definición de objetos: valores, vectores y DataFrames Tipos de variable (numéricas, de caracteres, lógicas) Lectura y escritura de archivos "],
["explicacion.html", "2.1 Explicación", " 2.1 Explicación https://cran.r-project.org/ 2.1.1 ¿Qué es R? Lenguaje para el procesamiento y análisis estadístico de datos Software Libre Sintaxis Básica: R base Sintaxis incremental1: El lenguaje se va ampliando por aportes de Universidades, investigadores/as, usuarios/as y empresas privadas, organizados en librerías (o paquetes) Comunidad web muy grande para realizar preguntas y despejar dudas. Por ejemplo, en el caso de Buenos Aires contamos con R-Ladies Buenos Aires y RenBaires. Gráficos con calidad de publicación fuente: https://gist.github.com/daroczig/3cf06d6db4be2bbe3368 https://www.rstudio.com/ Uno de los entornos más cómodos para utilizar el lenguaje R es el programa R studio. Rstudio es una empresa que produce productos asociados al lenguaje R, como el programa sobre el que corremos los comandos, y extensiones del lenguaje (librerías). El programa es gratuito y se puede bajar de la página oficial Pantalla Rstudio 2.1.2 Lógica sintáctica. 2.1.2.1 Definición de objetos Los Objetos/Elementos constituyen la categoría esencial del R. De hecho, todo en R es un objeto, y se almacena con un nombre específico que no debe poseer espacios. Un número, un vector, una función, la progresión de letras del abecedario, una base de datos, un gráfico, constituyen para R objetos de distinto tipo. Los objetos que vamos creando a medida que trabajamos pueden visualizarse en el panel derecho superior de la pantalla (el Environment). El operador &lt;- (Alt + Guión) sirve para definir un objeto. A la izquierda del &lt;- debe ubicarse el nombre que tomará el elemento a crear. Del lado derecho debe ir la definición del mismo. A &lt;- 1 Por ejemplo, podemos crear el elemento A, cuyo valor será 1. Para esto, debemos correr el código presionando Ctrl + Enter, con el cursor ubicado en cualquier parte de la línea. Al definir un elemento, el mismo queda guardado en el ambiente del programa, y podrá ser utilizado posteriormente para observar su contenido o para realizar una operación con el mismo. A ## [1] 1 A+6 ## [1] 7 Al correr una linea con el nombre del objeto, la consola del programa nos muestra su contenido. Entre corchetes observamos el número de orden del elemento en cuestión. Si corremos una operación, la consola nos muestra el resultado de la misma. El operador = es equivalente a &lt;-, pero en la práctica no se utiliza para la definición de objetos. B = 2 B ## [1] 2 &lt;- es un operador Unidireccional, es decir que: A &lt;- B implica que A va tomar como valor el contenido del objeto B, y no al revés. A &lt;- B A # Ahora A toma el valor de B, y B continúa conservando el mismo valor ## [1] 2 B ## [1] 2 2.1.3 R base Con R base nos referimos a los comandos básicos que vienen incorporados en el R, sin necesidad de cargar librerías. 2.1.3.1 Operadores lógicos: \\(&gt;\\) (mayor a-) \\(&gt;=\\) (mayor o igual a-) \\(&lt;\\) (menor a-) \\(&lt;=\\) (menor o igual a-) \\(==\\) (igual a-) \\(!=\\) (distinto a-) # Redefinimos los valores A y B A &lt;- 10 B &lt;- 20 # Realizamos comparaciones lógicas A &gt; B ## [1] FALSE A &gt;= B ## [1] FALSE A &lt; B ## [1] TRUE A &lt;= B ## [1] TRUE A == B ## [1] FALSE A != B ## [1] TRUE C &lt;- A != B C ## [1] TRUE Como muestra el último ejemplo, el resultado de una operación lógica puede almacenarse como el valor de un objeto. 2.1.3.2 Operadores aritméticos: #suma A &lt;- 5+6 A ## [1] 11 #Resta B &lt;- 6-8 B ## [1] -2 #cociente C &lt;- 6/2.5 C ## [1] 2.4 #multiplicacion D &lt;- 6*2.5 D ## [1] 15 2.1.3.3 Funciones: Las funciones son series de procedimientos estandarizados, que toman como imput determinados argumentos a fijar por el usuario, y devuelven un resultado acorde a la aplicación de dichos procedimientos. Su lógica de funcionamiento es: funcion(argumento1 = arg1, argumento2 = arg2) A lo largo del curso iremos viendo numerosas funciones, según lo requieran los distintos ejercicios. Sin embargo, veamos ahora algunos ejemplos para comprender su funcionamiento: paste() : concatena una serie de caracteres, pudiendo indicarse cómo separar a cada uno de ellos paste0(): concatena una serie de caracteres sin separar sum(): suma de todos los elementos de un vector mean() promedio aritmético de todos los elementos de un vector paste(&quot;Pega&quot;, &quot;estas&quot;, 4, &quot;palabras&quot;, sep = &quot; &quot;) ## [1] &quot;Pega estas 4 palabras&quot; #Puedo concatenar caracteres almacenados en objetos paste(A, B, C, sep = &quot;**&quot;) ## [1] &quot;11**-2**2.4&quot; # Paste0 pega los caracteres sin separador paste0(A, B, C) ## [1] &quot;11-22.4&quot; 1:5 ## [1] 1 2 3 4 5 sum(1:5) ## [1] 15 mean(1:5, na.rm = TRUE) ## [1] 3 2.1.3.4 Caracteres especiales R es sensible a mayúsculas y minúsculas, tanto para los nombres de las variables, como para las funciones y parámetros. Los espacios en blanco y los carriage return (enter) no son considerados por el lenguaje. Los podemos aprovechar para emprolijar el código y que la lectura sea más simple2. El numeral # se utiliza para hacer comentarios. Todo lo que se escribe después del # no es interpretado por R. Se debe utilizar un # por cada línea de código que se desea anular Los corchetes [] se utilizan para acceder a un objeto: en un vector[n° orden] en una tabla[fila, columna] en una lista[n° elemento] el signo $ también es un método de acceso. Particularmente, en los dataframes, nos permitira acceder a una determinada columna de una tabla Los paréntesis() se utilizan en las funciones para definir los parámetros. Las comas , se utilizan para separar los parametros al interior de una función. 2.1.4 Objetos: Existe una gran cantidad de objetos distintos en R, en lo que resepcta al curso trabajaremos principalmente con 3 de ellos: Valores Vectores Data Frames Listas 2.1.4.1 Valores Los valores y vectores pueden ser a su vez de distintas clases: Numeric A &lt;- 1 class(A) ## [1] &quot;numeric&quot; Character A &lt;- paste(&#39;Soy&#39;, &#39;una&#39;, &#39;concatenación&#39;, &#39;de&#39;, &#39;caracteres&#39;, sep = &quot; &quot;) A ## [1] &quot;Soy una concatenación de caracteres&quot; class(A) ## [1] &quot;character&quot; Factor A &lt;- factor(&quot;Soy un factor, con niveles fijos&quot;) class(A) ## [1] &quot;factor&quot; La diferencia entre un character y un factor es que el último tiene solo algunos valores permitidos (levels), con un orden interno predefinido (el cual, por ejemplo, se respetará a la hora de realizar un gráfico) 2.1.4.2 Vectores Para crear un vector utilizamos el comando c(), de combinar. C &lt;- c(1, 3, 4) C ## [1] 1 3 4 Podemos sumarle 2 a cada elemento del vector anterior C &lt;- C + 2 C ## [1] 3 5 6 O sumarle 1 al primer elemento, 2 al segundo, y 3 al tercer elemento del vector anterior D &lt;- C + 1:3 # esto es equivalente a hacer 3+1, 5+2, 6+9 D ## [1] 4 7 9 1:3 significa que queremos todos los números enteros desde 1 hasta 3. Podemos crear un vector que contenga las palabras: “Carlos”, “Federico”, “Pedro” E &lt;- c(&quot;Carlos&quot;, &quot;Federico&quot;, &quot;Pedro&quot;) E ## [1] &quot;Carlos&quot; &quot;Federico&quot; &quot;Pedro&quot; Para acceder a algún elemento del vector, podemos buscarlo por su número de orden, entre [ ] E[2] ## [1] &quot;Federico&quot; Si nos interesa almacenar dicho valor, al buscarlo lo asignamos a un nuevo objeto, dándole el nombre que deseemos elemento2 &lt;- E[2] elemento2 ## [1] &quot;Federico&quot; Para borrar un objeto del ambiente de trabajo, utilizamos el comando rm() rm(elemento2) elemento2 ## Error in eval(expr, envir, enclos): object &#39;elemento2&#39; not found También podemos cambiar el texto del segundo elemento de E, por el texto “Pablo” E[2] &lt;- &quot;Pablo&quot; E ## [1] &quot;Carlos&quot; &quot;Pablo&quot; &quot;Pedro&quot; 2.1.5 Data Frames Un Data Frame es una tabla de datos, donde cada columna representa una variable, y cada fila una observación. Este objeto suele ser central en el proceso de trabajo, y suele ser la forma en que se cargan datos externos para trabajar en el ambiente de R, y en que se exportan los resultados de nuestros trabajo. También se puede crear como la combinación de N vectores de igual tamaño. Por ejemplo, tomamos algunos valores del Indice de salarios INDICE &lt;- c(100, 100, 100, 101.8, 101.2, 100.73, 102.9, 102.4, 103.2) FECHA &lt;- c(&quot;Oct-16&quot;, &quot;Oct-16&quot;, &quot;Oct-16&quot;, &quot;Nov-16&quot;, &quot;Nov-16&quot;, &quot;Nov-16&quot;, &quot;Dic-16&quot;, &quot;Dic-16&quot;, &quot;Dic-16&quot;) GRUPO &lt;- c(&quot;Privado_Registrado&quot;,&quot;Público&quot;,&quot;Privado_No_Registrado&quot;, &quot;Privado_Registrado&quot;,&quot;Público&quot;,&quot;Privado_No_Registrado&quot;, &quot;Privado_Registrado&quot;,&quot;Público&quot;,&quot;Privado_No_Registrado&quot;) Datos &lt;- data.frame(INDICE, FECHA, GRUPO) Datos ## INDICE FECHA GRUPO ## 1 100.00 Oct-16 Privado_Registrado ## 2 100.00 Oct-16 Público ## 3 100.00 Oct-16 Privado_No_Registrado ## 4 101.80 Nov-16 Privado_Registrado ## 5 101.20 Nov-16 Público ## 6 100.73 Nov-16 Privado_No_Registrado ## 7 102.90 Dic-16 Privado_Registrado ## 8 102.40 Dic-16 Público ## 9 103.20 Dic-16 Privado_No_Registrado Tal como en un vector se ubica a los elementos mediante [ ], en un dataframe se obtienen sus elementos de la forma [fila, columna]. Otra opción es especificar la columna, mediante el operador $, y luego seleccionar dentro de esa columna el registro deseado mediante el número de orden. Datos$FECHA ## [1] Oct-16 Oct-16 Oct-16 Nov-16 Nov-16 Nov-16 Dic-16 Dic-16 Dic-16 ## Levels: Dic-16 Nov-16 Oct-16 Datos[3,2] ## [1] Oct-16 ## Levels: Dic-16 Nov-16 Oct-16 Datos$FECHA[3] ## [1] Oct-16 ## Levels: Dic-16 Nov-16 Oct-16 ¿que pasa si hacemos Datos$FECHA[3,2] ? Datos$FECHA[3,2] ## Error in `[.default`(Datos$FECHA, 3, 2): incorrect number of dimensions Nótese que el último comando tiene un número incorrecto de dimensiones, porque estamos refiriendonos 2 veces a la columna FECHA. Acorde a lo visto anteriormente, el acceso a los dataframes mediante [ ] puede utilizarse para realizar filtros sobre la base, especificando una condición para las filas. Por ejemplo, puedo utilizar los [ ] para conservar del dataframe Datos unicamente los registros con fecha de Diciembre 2016: Datos[Datos$FECHA==&quot;Dic-16&quot;,] ## INDICE FECHA GRUPO ## 7 102.9 Dic-16 Privado_Registrado ## 8 102.4 Dic-16 Público ## 9 103.2 Dic-16 Privado_No_Registrado La lógica del paso anterior sería: Accedo al dataframe Datos, pidiendo únicamente conservar las filas (por eso la condición se ubica a la izquierda de la ,) que cumplan el requisito de pertenecer a la categoría “Dic-16” de la variable FECHA. Aún más, podría aplicar el filtro y al mismo tiempo identificar una variable de interés para luego realizar un cálculo sobre aquella. Por ejemplo, podría calcular la media de los indices en el mes de Diciembre. ###Por separado Indices_Dic &lt;- Datos$INDICE[Datos$FECHA==&quot;Dic-16&quot;] Indices_Dic ## [1] 102.9 102.4 103.2 mean(Indices_Dic) ## [1] 102.8333 ### Todo junto mean(Datos$INDICE[Datos$FECHA==&quot;Dic-16&quot;]) ## [1] 102.8333 La lógica de esta sintaxis sería: “Me quedo con la variable INDICE, cuando la variable FECHA sea igual a &quot;Dic-16&quot;, luego calculo la media de dichos valores”. 2.1.6 Listas Contienen una concatenación de objetos de cualquier tipo. Así como un vector contiene valores, un dataframe contiene vectores, una lista puede contener dataframes, pero también vectores, o valores, y todo ello a la vez. superlista &lt;- list(A,B,C,D,E,FECHA, DF = Datos, INDICE, GRUPO) superlista ## [[1]] ## [1] Soy un factor, con niveles fijos ## Levels: Soy un factor, con niveles fijos ## ## [[2]] ## [1] -2 ## ## [[3]] ## [1] 3 5 6 ## ## [[4]] ## [1] 4 7 9 ## ## [[5]] ## [1] &quot;Carlos&quot; &quot;Pablo&quot; &quot;Pedro&quot; ## ## [[6]] ## [1] &quot;Oct-16&quot; &quot;Oct-16&quot; &quot;Oct-16&quot; &quot;Nov-16&quot; &quot;Nov-16&quot; &quot;Nov-16&quot; &quot;Dic-16&quot; &quot;Dic-16&quot; ## [9] &quot;Dic-16&quot; ## ## $DF ## INDICE FECHA GRUPO ## 1 100.00 Oct-16 Privado_Registrado ## 2 100.00 Oct-16 Público ## 3 100.00 Oct-16 Privado_No_Registrado ## 4 101.80 Nov-16 Privado_Registrado ## 5 101.20 Nov-16 Público ## 6 100.73 Nov-16 Privado_No_Registrado ## 7 102.90 Dic-16 Privado_Registrado ## 8 102.40 Dic-16 Público ## 9 103.20 Dic-16 Privado_No_Registrado ## ## [[8]] ## [1] 100.00 100.00 100.00 101.80 101.20 100.73 102.90 102.40 103.20 ## ## [[9]] ## [1] &quot;Privado_Registrado&quot; &quot;Público&quot; &quot;Privado_No_Registrado&quot; ## [4] &quot;Privado_Registrado&quot; &quot;Público&quot; &quot;Privado_No_Registrado&quot; ## [7] &quot;Privado_Registrado&quot; &quot;Público&quot; &quot;Privado_No_Registrado&quot; Para acceder un elemento de una lista, podemos utilizar el operador $, que se puede usar a su vez de forma iterativa. superlista$DF$FECHA[2] ## [1] Oct-16 ## Levels: Dic-16 Nov-16 Oct-16 2.1.7 Ambientes de trabajo Hay algunas cosas que tenemos que tener en cuenta respecto del orden del ambiente en el que trabajamos: Working Directory: Es el directorio de trabajo. Pueden ver el suyo con getwd(), es hacia donde apunta el código, por ejemplo, si quieren leer un archivo, la ruta del archivo tiene que estar explicitada como el recorrido desde el Working Directory. Environment: Esto engloba tanto la información que tenemos cargada en Data y Values, como las librerías que tenemos cargadas mientras trabajamos. Es importante que mantengamos bien delimitadas estas cosas entre diferentes trabajos, sino: El directorio queda referido a un lugar específico en nuestra computadora. Si se lo compartimos a otro se rompe Si cambiamos de computadora se rompe Si lo cambiamos de lugar se rompe Si primero abrimos otro script se rompe Tenemos mezclados resultados de diferentes trabajos: Nunca sabemos si esa variable/tabla/lista se creo en ese script y no otro Perdemos espacio de la memoria No estamos seguros de que el script cargue todas las librerías que necesita Rstudio tiene una herramienta muy útil de trabajo que son los proyectos. Estos permiten mantener un ambiente de trabajo delimitado por cada uno de nuestros trabajos. Es decir: El directorio de trabajo se refiere a donde esta ubicado el archivo .Rproj El Environment es específico de nuestro proyecto. Un proyecto no es un sólo script, sino toda una carpeta de trabajo. logo Rpoject Para crearlo, vamos al logo de nuevo projecto (Arriba a la derecha de la panatalla), y elegimos la carpeta de trabajo. 2.1.8 Tipos de archivos de R Script: Es un archivo de texto plano, donde podemos poner el código que utilizamos para preservarlo Rnotebook: También sirve para guardar el código, pero a diferencia de los scripts, se puede compilar, e intercalar código con resultados Rproject: Es un archivo que define la metadata del proyecto RDS y Rdata: Dos formatos de archivos propios de R para guardar datos. Más allá de los comandos elementales, comandos más sofisticados tienen muchas versiones, y algunas quedan en desuso en el tiempo.↩ veremos que existen ciertas excepciones con algunos paquetes más adelante.↩ "],
["practica-guiada.html", "2.2 Práctica Guiada", " 2.2 Práctica Guiada 2.2.1 Instalación de paquetes complementarios al R Base Hasta aquí hemos visto múltiples funciones que están contenidas dentro del lenguaje básico de R. Ahora bien, al tratarse de un software libre, los usuarios de R con más experiencia contribuyen sistemáticamente a expandir este lenguaje mediante la creación y actualización de paquetes complementarios. Lógicamente, los mismos no están incluidos en la instalación inicial del programa, pero podemos descargarlos e instalarlos al mismo tiempo con el siguiente comando: install.packages(&quot;nombre_del_paquete&quot;) Resulta recomendable ejecutar este comando desde la consola ya que sólo necesitaremos correrlo una vez en nuestra computadora. Al ejecutar el mismo, se descargarán de la pagina de CRAN los archivos correspondientes al paquete hacia el directorio en donde hayamos instalado el programa. Típicamente los archivos se encontrarán en C:\\Program Files\\R\\R-3.5.0\\library\\, siempre con la versión del programa correspondiente. Una vez instalado el paquete, cada vez que abramos una nueva sesión de R y querramos utilizar el mismo debemos cargarlo al ambiente de trabajo mediante la siguiente función: library(nombre_del_paquete) Nótese que al cargar/activar el paquete no son necesarias las comillas. 2.2.2 Lectura y escritura de archivos 2.2.2.1 .csv y .txt Hay muchas funciones para leer archivos de tipo .txt y .csv. La mayoría sólo cambia los parámetros que vienen por default. Es importante tener en cuenta que una base de datos que proviene de archivos .txt, o .csv puede presentar diferencias en cuanto a los siguientes parámetros: encabezado delimitador (,, tab, ;) separador decimal dataframe &lt;- read.delim(file, header = TRUE, sep = &quot;\\t&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;.&quot;, fill = TRUE, comment.char = &quot;&quot;, ...) Ejemplo. Levantar la base de sueldos de funcionarios En el parametro file tengo que especificar el nombre completo del archivo, incluyendo el directorio donde se encuentra. Lo más sencillo es abrir comillas, apretar Tab y se despliega el menú de las cosas que tenemos en el directorio de trabajo. Si queremos movernos hacia arriba, agregamos ../ sueldos_funcionarios &lt;- read.table(file = &#39;../fuentes/sueldo_funcionarios_2019.csv&#39;,sep=&quot;,&quot;, header = TRUE) sueldos_funcionarios[1:10,] ## cuil anio mes funcionario_apellido funcionario_nombre ## 1 20-17692128-6 2019 1 RODRIGUEZ LARRETA HORACIO ANTONIO ## 2 20-17735449-0 2019 1 SANTILLI DIEGO CESAR ## 3 27-24483014-0 2019 1 ACUÑA MARIA SOLEDAD ## 4 20-13872301-2 2019 1 ASTARLOA GABRIEL MARIA ## 5 20-25641207-2 2019 1 AVOGADRO ENRIQUE LUIS ## 6 27-13221055-7 2019 1 BOU PEREZ ANA MARIA ## 7 27-13092400-5 2019 1 FREDA MONICA BEATRIZ ## 8 20-17110752-1 2019 1 MACCHIAVELLI EDUARDO ALBERTO ## 9 20-22293873-3 2019 1 MIGUEL FELIPE OSCAR ## 10 20-14699669-9 2019 1 MOCCIA FRANCO ## repartición asignacion_por_cargo_i ## 1 Jefe de Gobierno 197745.8 ## 2 Vicejefatura de Gobierno 197745.8 ## 3 Ministerio de Educación e Innovación 224516.6 ## 4 Procuración General de la Ciudad de Buenos Aires 224516.6 ## 5 Ministerio de Cultura 224516.6 ## 6 Ministerio de Salud 224516.6 ## 7 Sindicatura General de la Ciudad de Buenos Aires 224516.6 ## 8 Ministerio de Ambiente y Espacio Público 224516.6 ## 9 Jefatura de Gabinete de Ministros 224516.6 ## 10 Ministerio de Desarrollo Urbano y Transporte 224516.6 ## aguinaldo_ii total_salario_bruto_i_._ii observaciones ## 1 0 197745.8 ## 2 0 197745.8 ## 3 0 224516.6 ## 4 0 224516.6 ## 5 0 224516.6 ## 6 0 224516.6 ## 7 0 224516.6 ## 8 0 224516.6 ## 9 0 224516.6 ## 10 0 224516.6 Como puede observarse aquí, la base cuenta con 94 registros y 10 variables. Al trabajar con bases de microdatos, resulta conveniente contar con algunos comandos para tener una mirada rápida de la base, antes de comenzar a realizar los procesamientos que deseemos. Veamos algunos de ellos: #view(sueldos_funcionarios) names(sueldos_funcionarios) ## [1] &quot;cuil&quot; &quot;anio&quot; ## [3] &quot;mes&quot; &quot;funcionario_apellido&quot; ## [5] &quot;funcionario_nombre&quot; &quot;repartición&quot; ## [7] &quot;asignacion_por_cargo_i&quot; &quot;aguinaldo_ii&quot; ## [9] &quot;total_salario_bruto_i_._ii&quot; &quot;observaciones&quot; summary(sueldos_funcionarios) ## cuil anio mes funcionario_apellido ## 20-13872301-2: 3 Min. :2019 Min. :1.00 ACUÑA : 3 ## 20-14699669-9: 3 1st Qu.:2019 1st Qu.:2.00 ASTARLOA : 3 ## 20-16891528-5: 3 Median :2019 Median :3.00 AVELLANEDA: 3 ## 20-16891539-0: 3 Mean :2019 Mean :3.34 AVOGADRO : 3 ## 20-17110752-1: 3 3rd Qu.:2019 3rd Qu.:5.00 BENEGAS : 3 ## 20-17692128-6: 3 Max. :2019 Max. :6.00 BOU PEREZ : 3 ## (Other) :76 (Other) :76 ## funcionario_nombre ## ANA MARIA : 3 ## BRUNO GUIDO : 3 ## CHRISTIAN : 3 ## DIEGO CESAR : 3 ## DIEGO HERNAN : 3 ## EDUARDO ALBERTO: 3 ## (Other) :76 ## repartición ## Consejo de los Derechos de Niñas, Niños y Adoles - Presidencia: 3 ## Ente de Turismo Ley Nº 2627 : 3 ## Jefatura de Gabinete de Ministros : 3 ## Jefe de Gobierno : 3 ## Ministerio de Ambiente y Espacio Público : 3 ## Ministerio de Cultura : 3 ## (Other) :76 ## asignacion_por_cargo_i aguinaldo_ii total_salario_bruto_i_._ii ## Min. :197746 Min. : 0 Min. :197746 ## 1st Qu.:217520 1st Qu.: 0 1st Qu.:217805 ## Median :226866 Median : 0 Median :226866 ## Mean :224718 Mean : 14843 Mean :239560 ## 3rd Qu.:231168 3rd Qu.: 0 3rd Qu.:248033 ## Max. :249662 Max. :113433 Max. :340300 ## ## observaciones ## :93 ## baja 28/2/2019: 1 ## ## ## ## ## head(sueldos_funcionarios)[,1:5] ## cuil anio mes funcionario_apellido funcionario_nombre ## 1 20-17692128-6 2019 1 RODRIGUEZ LARRETA HORACIO ANTONIO ## 2 20-17735449-0 2019 1 SANTILLI DIEGO CESAR ## 3 27-24483014-0 2019 1 ACUÑA MARIA SOLEDAD ## 4 20-13872301-2 2019 1 ASTARLOA GABRIEL MARIA ## 5 20-25641207-2 2019 1 AVOGADRO ENRIQUE LUIS ## 6 27-13221055-7 2019 1 BOU PEREZ ANA MARIA 2.2.2.2 Excel Para leer y escribir archivos excel podemos utilizar los comandos que vienen con la librería openxlsx # install.packages(&quot;openxlsx&quot;) # por única vez library(openxlsx) #activamos la librería # creamos una tabla cualquiera de prueba x &lt;- 1:10 y &lt;- 11:20 tabla_de_R &lt;- data.frame(x,y) # escribimos el archivo write.xlsx(x = tabla_de_R, file = &quot;../resultados/archivo.xlsx&quot;, row.names = FALSE) # Donde lo guardó? Hay un directorio por default en caso de que no hayamos definido alguno. # getwd() # Si queremos exportar multiples dataframes a un Excel, debemos armar previamente una lista de ellos. Cada dataframe se guardará en una pestaña de excel, cuyo nombre corresponderá al que definamos para cada Dataframe a la hora de crear la lista. Lista_a_exportar &lt;- list(&quot;sueldos funcionarios&quot; = sueldos_funcionarios, &quot;Tabla Numeros&quot; = tabla_de_R) write.xlsx(x = Lista_a_exportar, file = &quot;../resultados/archivo_2_hojas.xlsx&quot;, row.names = FALSE) # leemos el archivo especificando la ruta (o el directorio por default) y el nombre de la hoja que contiene los datos Indices_Salario &lt;- read.xlsx(xlsxFile = &quot;../resultados/archivo_2_hojas.xlsx&quot;, sheet = &quot;sueldos funcionarios&quot;) # alternativamente podemos especificar el número de orden de la hoja que deseamos levantar Indices_Salario &lt;- read.xlsx(xlsxFile = &quot;../resultados/archivo_2_hojas.xlsx&quot;, sheet = 1) Indices_Salario[1:10,] ## cuil anio mes funcionario_apellido funcionario_nombre ## 1 20-17692128-6 2019 1 RODRIGUEZ LARRETA HORACIO ANTONIO ## 2 20-17735449-0 2019 1 SANTILLI DIEGO CESAR ## 3 27-24483014-0 2019 1 ACUÑA MARIA SOLEDAD ## 4 20-13872301-2 2019 1 ASTARLOA GABRIEL MARIA ## 5 20-25641207-2 2019 1 AVOGADRO ENRIQUE LUIS ## 6 27-13221055-7 2019 1 BOU PEREZ ANA MARIA ## 7 27-13092400-5 2019 1 FREDA MONICA BEATRIZ ## 8 20-17110752-1 2019 1 MACCHIAVELLI EDUARDO ALBERTO ## 9 20-22293873-3 2019 1 MIGUEL FELIPE OSCAR ## 10 20-14699669-9 2019 1 MOCCIA FRANCO ## repartición asignacion_por_cargo_i ## 1 Jefe de Gobierno 197745.8 ## 2 Vicejefatura de Gobierno 197745.8 ## 3 Ministerio de Educación e Innovación 224516.6 ## 4 Procuración General de la Ciudad de Buenos Aires 224516.6 ## 5 Ministerio de Cultura 224516.6 ## 6 Ministerio de Salud 224516.6 ## 7 Sindicatura General de la Ciudad de Buenos Aires 224516.6 ## 8 Ministerio de Ambiente y Espacio Público 224516.6 ## 9 Jefatura de Gabinete de Ministros 224516.6 ## 10 Ministerio de Desarrollo Urbano y Transporte 224516.6 ## aguinaldo_ii total_salario_bruto_i_._ii observaciones ## 1 0 197745.8 ## 2 0 197745.8 ## 3 0 224516.6 ## 4 0 224516.6 ## 5 0 224516.6 ## 6 0 224516.6 ## 7 0 224516.6 ## 8 0 224516.6 ## 9 0 224516.6 ## 10 0 224516.6 "],
["probabilidad-y-estadistica.html", "Capítulo-3 Probabilidad y Estadística", " Capítulo-3 Probabilidad y Estadística Esta clase es un repaso de los rudimentos de probabilidad y estadística. El objetivo es obtener las herramientas básicas para la interpretación de resultados estadísticos. Introducción a probabilidad Introducción a distribuciones El problema de la inversión Estadística Población y muestra Estimadores puntuales, tests de hipótesis Boxplots, histogramas y kernels "],
["explicacion-1.html", "3.1 Explicación", " 3.1 Explicación 3.1.1 Probabilidad Previo a estudiar las herramientas de la estadística descriptiva, es necesario hacer un breve resumen de algunos conceptos fundamentales de probabilidad 3.1.1.1 Marco conceptual El análisis de las probabilidades parte de un proceso generador de datos entendido como cualquier fenómeno que produce algún tipo de información de forma sistemática. Cada iteración de este proceso produce información, que podemos interpretar como un resultado. Existe un conjunto de posibles resultados, que definimos como espacio muestral. Un evento es el conjunto de resultados ocurridos. En este marco, la probabilidad es un atributo de los eventos. Es la forma de medir los eventos tal que, siguiendo la definición moderna de probabilidad: \\(P(A) \\geq 0 \\forall \\ A \\subseteq \\Omega\\) \\(P(\\Omega)=1\\) \\(P(A\\cup B) = P(A) + P(B)\\ si\\ A \\cap B = \\emptyset\\) ejemplo, tiramos un dado y sale tres Espacio muestral: 1,2,3,4,5,6 Resultado: 3 Evento: impar (el conjunto 1,3,5) 3.1.1.2 Distribución de probabilidad La distribución de probabilidad hace referencia a los posibles valores teóricos de cada uno de los resultados pertenecientes al espacio muestral. Existen dos tipos de distribuciones, dependiendo si el espacio muestral es o no numerable. 3.1.1.2.1 Distribuciones discretas Sigamos con el ejemplo de dado. Podríamos definir la distribución de probabilidad, si el dado no está cargado, como: ## # A tibble: 6 x 2 ## valor probabilidad ## &lt;int&gt; &lt;chr&gt; ## 1 1 1/6 ## 2 2 1/6 ## 3 3 1/6 ## 4 4 1/6 ## 5 5 1/6 ## 6 6 1/6 Como el conjunto de resultados posibles es acotado, podemos definirlo en una tabla, esta es una distribución discreta. 3.1.1.2.2 Distribuciones continuas ¿Qué pasa cuando el conjunto de resultados posibles es tan grande que no se puede enumerar la probabilidad de cada caso? Si, por definición o por practicidad, no se puede enumerar cada caso, lo que tenemos es una distribución continua Por ejemplo, la altura de la población En este caso, no podemos definir en una tabla la probabilidad de cada uno de los posibles valores. de hecho, la probabilidad puntual es 0. Sin embargo, sí podemos definir una función de probabilidad, la densidad. Según qué función utilicemos, cambiará la forma de la curva. Por ejemplo: Una distribución de probabilidad se caracteriza por sus parámetros. Por ejemplo, la distribución normal se caracteriza por su esperanza y su varianza (o desvío estándar) 3.1.2 Estadística 3.1.2.1 El problema de la inversión El problema de la probabilidad se podría pensar de la siguiente forma: Vamos a partir de un proceso generador de datos Para calcular su distribución de probabilidad, los parámetros que caracterizan a ésta, y a partir de allí, Calcular la probabilidad de que, al tomar una muestra, tenga ciertos eventos. El problema de la estadística es exactamente el contrario: Partimos de una muestra para Inferir cuál es la distribución de probabilidad, y los parámetros que la caracterizan Para finalmente poder sacar conclusiones sobre el proceso generador de datos 3.1.2.1.1 Población y muestra En este punto podemos hacer la distinción entre población y muestra Población: El universo en estudio. Puede ser: finita: Los votantes en una elección. infinita: El lanzamiento de una moneda. Muestra: subconjunto de n observaciones de una población. Solemos utilizar las mayúsculas (N) para la población y las minúsculas (n) para las muestras 3.1.2.1.2 Parámetros y Estimadores Como dijimos, los parámetros describen a la función de probabilidad. Por lo tanto hacen referencia a los atributos de la población. Podemos suponer que son constantes. Un estimador es un estadístico (esto es, una función de la muestra) usado para estimar un parámetro desconocido de la población. 3.1.2.1.3 Ejemplo. La media Esperanza o Media Poblacional: \\[ \\mu = E(x)= \\sum_{i=1}^N x_ip(x_i) \\] Media muestral: \\[ \\bar{X}= \\sum_{i=1}^n \\frac{Xi}{n} \\] Como no puedo conocer \\(\\mu\\), lo estimo mediante \\(\\bar{X}\\) 3.1.2.2 Estimación puntual, Intervalos de confianza y Tests de hipótesis El estimador \\(\\bar{X}\\) nos devuelve un número. Esto es una inferencia de cuál creemos que es la media. Pero no es seguro que esa sea realmente la media. Esto es lo que denominamos estimación puntual. También podemos estimar un intervalo, dentro del cual consideramos que se encuentra la media poblacional. La ventaja de esta metodología es que podemos definir la probabilidad de que el parámetro poblacional realmente esté dentro de este intervalo. Esto se conoce como intervalos de confianza. Por su parte, también podemos calcular la probabilidad de que el parámetro poblacional sea mayor, menor o igual a un cierto valor. Esto es lo que se conoce como test de hipótesis. En el fondo, los intervalos de confianza y los tests de hipótesis se construyen de igual manera. Son funciones que se construyen a partir de los datos, que se comparan con distribuciones conocidas, teóricas. 3.1.2.2.1 Definición de los tests Los tests se construyen con dos hipótesis: La hipótesis nula \\(H_0\\), y la hipótesis alternativa, \\(H_1\\). Lo que buscamos es ver si hay evidencia suficiente para rechazar la hipótesis nula. Por ejemplo, si queremos comprobar si la media poblacional, \\(\\mu\\) de una distribución es mayor a \\(X_i\\), haremos un test con las siguientes hipótesis: \\(H_0: \\mu = X_i\\) \\(H_1: \\mu &gt; X_i\\) Si la evidencia es lo suficientemente fuerte, podremos rechazar la hipótesis \\(H_0\\), pero no afirmar la hipótesis \\(H_1\\) 3.1.2.2.2 Significatividad en los tests Muchas veces decimos que algo es “estadísticamente significativo”. Detrás de esto se encuentra un test de hipótesis que indica que hay una suficiente significativdad estadística. La significatividad estadística, representada con \\(\\alpha\\), es la probabilidad de rechazar \\(H_0\\) cuando en realidad es cierta. Por eso, cuanto más bajo el valor de \\(\\alpha\\), más seguros estamos de no equivocarnos. Por lo general testeamos con valores de alpha de 1%, 5% y 10%, dependiendo del área de estudio. El p-valor es la mínima significatividad para la que rechazo el test. Es decir, cuanto más bajo es el p-valor, más seguros estamos de rechazar \\(H_0\\). El resultado de un test está determinado por: La fuerza de la evidencia empírica: Si nuestra duda es si la media poblacional es mayor a, digamos, 10, y la media muestral es 11, no es lo mismo que si es 100, 1000 o 10000. El tamaño de la muestra: En las fórmulas que definen los test siempre juega el tamaño de la muestra: cuanto más grande es, más seguros estamos de que el resultado no es producto del mero azar. La veracidad de los supuestos: Otra cosa importante es que los test asumen ciertas cosas: Normalidad en los datos. Que conocemos algún otro parámetro de la distribución, como la varianza. Que los datos son independientes entre sí, Etc. Cada Test tiene sus propios supuestos. Por eso a veces, luego de hacer un test, hay que hacer otros tests para validar que los supuestos se cumplen. Lo primero, la fuerza de la evidencia, es lo que más nos importa, y no hay mucho por hacer. El tamaño de la muestra es un problema, porque si la muestra es muy chica, entonces podemos no llegar a conclusiones significativas aunque sí ocurra aquello que queríamos probar. Sin embargo, el verdadero problema en La era del big data es que tenemos muestras demasiado grandes, por lo que cualquier test, por más mínima que sea la diferencia, puede dar significativo. Por ejemplo, podemos decir que la altura promedio en Argentina es 1,74. Pero si hacemos un test, utilizando como muestra 40 millones de personas, vamos a rechazar que ese es el valor, porque en realidad es 1,7401001. En términos de lo que nos puede interesar, 1,74 sería válido, pero estadísticamente rechazaríamos. Finalmente, según la información que tengamos de la población y cuál es el problema que queremos resolver, vamos a tener que utilizar distintos tipos de tests. La cantidad de tests posibles es ENORME, y escapa al contenido de este curso, así como sus fórmulas. A modo de ejemplo, les dejamos el siguiente machete: fuente: http://abacus.bates.edu/~ganderso/biology/resources/statistics.html 3.1.3 Algunos estimadores importantes 3.1.3.1 Medidas de centralidad Media \\[ \\bar{X}= \\sum_{i=1}^n \\frac{Xi}{n} \\] Mediana: Es el valor que parte la distribución a la mitad Moda La moda es el valor más frecuente de la distribución 3.1.3.2 Cuantiles Así como dijimos que la mediana es el valor que deja al 50% de los datos de un lado y al 50% del otro, podemos generalizar este concepto a cualquier X%. Esto son los cuantiles. El cuantil x, es el valor tal que queda un x% de la distribución a izquierda, y 1-x a derecha. Algunos de los más utilizados son el del 25%, también conocido como \\(Q_1\\) (el cuartil 1), el \\(Q_2\\) (la mediana) y el \\(Q_3\\) (el cuartil 3), que deja el 75% de los datos a su derecha. Veamos cómo se ven en la distribución de arriba. 3.1.3.3 Desvío estándar El desvío estándar es una medida de dispersión de los datos, que indica cuánto se suelen alejar de la media. 3.1.4 Gráficos estadísticos Cerramos la explicación con algunos gráficos que resultan útiles para entender las propiedades estadísticas de los datos. 3.1.4.1 Boxplot El Boxplot es muy útil para describir una distribución y para detectar outliers. Reúne los principales valores que caracterizan a una distribución: \\(Q_1\\) \\(Q_2\\) (la mediana) \\(Q_3\\) el rango intercuarítlico \\(Q_3 - Q_1\\), que define el centro de la distribución Outliers, definidos como aquellos puntos que se encuentran a más de 1,5 veces el rango intercuartílico del centro de la distribución. Veamos qué pinta tienen los boxplot de números generados aleatoriamente a partir de tres distribuciones que ya vimos. En este caso, sólo tomaremos 15 valores de cada distribución Algunas cosas que resaltan: la distribución \\(\\chi^2\\) no toma valores en los negativos. La normal esta más concentrada en el centro de la distribución. Podemos generar 100 números aleatorios en lugar de 15: Cuando generamos 100 valores en lugar de 15, tenemos más chances de agarrar un punto alejado en la distribución. De esta forma podemos apreciar las diferencias entre la distribución normal y la T-student. También podemos volver a repasar qué efecto generan los distintos parámetros. Por ejemplo: 3.1.4.2 Histograma Otra forma de analizar una distribución es mediante los histogramas: En un histograma agrupamos las observaciones en rangos fijos de la variable y contamos la cantidad de ocurrencias. Cuanto más alta es una barra, es porque más observaciones se encuentran en dicho rango. Veamos el mismo ejemplo que arriba, pero con histogramas: 3.1.4.3 Kernel Los Kernels son simplemente un suavizado sobre los histogramas. 3.1.4.4 Violin plots Combinando la idea de Kernels y Boxplots, se crearon los violin plots, que simplemente muestran a los kernels duplicados. 3.1.5 Bibliografía de consulta Quién quiera profundizar en estos temas, puede ver los siguientes materiales: https://seeing-theory.brown.edu/ https://lagunita.stanford.edu/courses/course-v1:OLI+ProbStat+Open_Jan2017/about Jay L. Devore, “Probabilidad y Estadística para Ingeniería y Ciencias”, International Thomson Editores. https://inferencialitm.files.wordpress.com/2018/04/probabilidad-y-estadistica-para-ingenieria-y-ciencias-devore-7th.pdf "],
["practica-guiada-1.html", "3.2 Práctica Guiada", " 3.2 Práctica Guiada library(tidyverse) 3.2.1 Generación de datos aleatorios Para generar datos aleatorios, usamos las funciones: rnorm para generar datos que surgen de una distribución normal rt para generar datos que surgen de una distribución T-student rchisq para generar datos que surgen de una distribución Chi cuadrado Pero antes, tenemos que fijar la semilla para que los datos sean reproducibles set.seed(1234) rnorm(n = 15, mean = 0, sd = 1 ) ## [1] -1.20706575 0.27742924 1.08444118 -2.34569770 0.42912469 ## [6] 0.50605589 -0.57473996 -0.54663186 -0.56445200 -0.89003783 ## [11] -0.47719270 -0.99838644 -0.77625389 0.06445882 0.95949406 rt(n = 15, df=1 ) ## [1] -0.363717710 -1.603466805 -0.388596796 -0.588007490 0.007839245 ## [6] 14.690527710 -1.863488555 0.022667470 -2.084247299 -0.249237745 ## [11] -1.311594174 -3.569055208 -2.490838240 -3.848779244 -4.271087169 rchisq(n = 15,df=1) ## [1] 0.5317744 1.4263809 4.2797098 0.2184660 0.6923773 0.0455256 3.1902100 ## [8] 0.2949942 0.5403827 0.1543732 0.8639196 0.1417290 1.1386091 0.2966193 ## [15] 0.5110879 Para poder ver rápidamente de qué se tratan los valores, podemos usar el comando plot plot(rnorm(n = 15,mean = 0, sd = 1 )) plot(rt(n = 15,df=1 )) plot(rchisq(n = 15,df=1)) Noten que el eje X es el índice de los valores, es decir que no agrega información. 3.2.2 Tests Utilicemos ahora datos reales. Los datos salen de https://data.buenosaires.gob.ar/dataset/femicidios Vamos a ver ahora las estadisticas de Buenos Aires sobre la cantidad de femicidios por grupo etario. Es interesante preguntarse si hay más femicidios para cierto rango etario. femicidios &lt;- read_csv(file = &#39;../fuentes/vict_fem_annio__g_edad_limpio.csv&#39;) femicidios ## # A tibble: 19 x 3 ## anio cantidad_femicidios grupo_edad ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2015 1 0 - 15 ## 2 2015 2 16 - 20 ## 3 2015 5 21 - 40 ## 4 2015 3 41 - 60 ## 5 2015 - 61 y más ## 6 2015 1 Ignorado ## 7 2016 2 0 - 15 ## 8 2016 3 16 - 20 ## 9 2016 4 21 - 40 ## 10 2016 1 41 - 60 ## 11 2016 2 61 y más ## 12 2016 2 Ignorado ## 13 2017 … 0 - 15 ## 14 2017 … 16 - 20 ## 15 2017 … 21 - 40 ## 16 2017 … 41 - 60 ## 17 2017 … 61 y más ## 18 2017 … Ignorado ## 19 2017 9 TOTAL Fijense que las estadísitcas no están desagregadas por rango etario para 2017, que en caso de que haya 0 femicidios pusieron ‘-’ en lugar de 0. Además, como tenemos pocos datos, es mejor hacer un test que compare sólamente dos grupos. Vamos a reorganizar la información para corregir todas estas cosas femicidios &lt;- femicidios %&gt;% filter(anio!=2017, grupo_edad !=&#39;Ignorado&#39;) %&gt;% #Sacamos al 2017 y los casos donde se ignora la edad mutate(cantidad_femicidios = case_when(cantidad_femicidios==&#39;-&#39; ~ 0, # reemplazamos el - por 0 TRUE ~as.numeric(cantidad_femicidios)), # y convertimos la variable en numerica grupo_edad = case_when(grupo_edad %in% c(&#39;0 - 15&#39;,&#39;16 - 20&#39;,&#39;21 - 40&#39;) ~ &#39;0-40&#39;, # agrupamos para tener sólo dos grupos grupo_edad %in% c(&#39;41 - 60&#39;,&#39;61 y más&#39;) ~ &#39;41 y más&#39;)) %&gt;% group_by(grupo_edad) %&gt;% summarise(cantidad_femicidios= sum(cantidad_femicidios)) # sumamos los años y grupos para tener datos agregados femicidios ## # A tibble: 2 x 2 ## grupo_edad cantidad_femicidios ## &lt;chr&gt; &lt;dbl&gt; ## 1 0-40 17 ## 2 41 y más 6 Con esta tabla de contingencia podemos hacer un test de hipótesis. ¿Cuál usamos? Nos fijamos en el machete, o googleamos, y vemos que como queremos comparar la cantidad de casos por grupos categóricos, tenemos que usar el test Chi. \\(H_0\\) No hay asociación entre las variables \\(H_1\\) Hay asociación entre las variables La idea es que tenemos dos variables: El rango etario y la cantidad de femicidios chisq.test(femicidios$cantidad_femicidios) ## ## Chi-squared test for given probabilities ## ## data: femicidios$cantidad_femicidios ## X-squared = 5.2609, df = 1, p-value = 0.02181 Noten que el resultado lo dan en términos del p-valor. Como el valor es bajo, menor a 0.05, entonces podemos rechazar que no existe relación. O en otros términos, pareciera que la diferencia es significativa estadísticamente. 3.2.3 Descripción estadística de los datos Volveremos a ver los datos de sueldos de funcionarios sueldos &lt;- read_csv(&#39;../fuentes/sueldo_funcionarios_2019.csv&#39;) Con el comando summary podemos ver algunos de los principales estadísticos de resumen summary(sueldos$asignacion_por_cargo_i) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 197746 210061 226866 225401 231168 249662 3.2.4 Gráficos estadísticos No nos vamos a detener demasiado a ver cómo hacer los gráficos de resumen, porque la próxima clase veremos como realizar gráficos de mejor calidad, como los presentados en las notas de clase. A modo de ejemplo, dejamos los comandos de R base para realizar gráficos. boxplot(sueldos$asignacion_por_cargo_i) hist(sueldos$asignacion_por_cargo_i) plot(density(sueldos$asignacion_por_cargo_i)) "],
["visualizacion-de-la-informacion.html", "Capítulo-4 Visualización de la información", " Capítulo-4 Visualización de la información En esta clase veremos como realizar gráficos en R, tanto los comandos básicos como utilizando la librería GGPLOT. Gráficos básicos de R (función “plot”): Comandos para la visualización ágil de la información Gráficos elaborados en R (función “ggplot”): Gráficos de línea, barras, Boxplots y distribuciones de densidad Parámetros de los gráficos: Leyendas, ejes, títulos, notas, colores Gráficos con múltiples cruces de variables. "],
["explicacion-2.html", "4.1 Explicación", " 4.1 Explicación 4.1.1 Gráficos Básicos en R Rbase tiene algunos comandos genéricos para realizar gráficos, que se adaptan al tipo de información que se le pide graficar, por ejemplo: plot() hist() # iris es un set de datos clásico, que ya viene incorporado en R iris[10,] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 10 4.9 3.1 1.5 0.1 setosa plot(iris) #Al especificar una variable, puedo ver el valor que toma cada uno de sus registros (Index) plot(iris$Sepal.Length,type = &quot;p&quot;) # Un punto por cada valor plot(iris$Sepal.Length,type = &quot;l&quot;) # Una linea que una cada valor plot(iris$Sepal.Length,type = &quot;b&quot;) #Ambas hist(iris$Sepal.Length, col = &quot;lightsalmon1&quot;, main = &quot;Histograma&quot;) 4.1.1.1 png La función png() nos permite grabar una imagen en el disco. Lleva como argumento principal la ruta completa a donde se desea guardar la misma, incluyendo el nombre que queremos dar al archivo. A su vez pueden especificarse otros argumetnos como el ancho y largo de la imagen, entre otros. ruta_archivo &lt;- &quot;../resultados/grafico1.PNG&quot; ruta_archivo ## [1] &quot;../resultados/grafico1.PNG&quot; png(ruta_archivo) plot(iris$Sepal.Length,type = &quot;b&quot;) dev.off() ## png ## 2 La función png() abre el dispositivo de imagen en el directorio especificado. Luego creamos el gráfico que deseamos (o llamamos a uno previamente construido), el cual se desplegará en la ventana inferior derecha de la pantalla de Rstudio. Finalmente con dev.off() se cierra el dispositivo y se graban los gráficos. Los gráficos del R base son útiles para escribir de forma rápida y obtener alguna información mientras trabajamos. Muchos paquetes estadísticos permiten mostrar los resultados de forma gráfica con el comando plot (por ejemplo, las regresiones lineales lm()). Sin embargo, existen librerías mucho mejores para crear gráficos de nivel de publicación. La más importante es ggplot2, que a su vez tiene extensiones mediante otras librerías. 4.1.2 Ggplot2 ggplot tiene su sintaxis propia. La idea central es pensar los gráficos como una sucesión de capas, que se construyen una a la vez. El operador + nos permite incorporar nuevas capas al gráfico. El comando ggplot() nos permite definir la fuente de datos y las variables que determinaran los ejes del grafico (x,y), así como el color y la forma de las líneas o puntos,etc. Las sucesivas capas nos permiten definir: Uno o más tipos de gráficos (de columnas, geom_col(), de línea, geom_line(), de puntos,geom_point(), boxplot, geom_boxplot()) Títulos labs() Estilo del gráfico theme() Escalas de los ejes scale_y_continuous,scale_x_discrete División en subconjuntos facet_wrap(),facet_grid() ggplot tiene muchos comandos, y no tiene sentido saberlos de memoria, es siempre útil reutilizar gráficos viejos y tener a mano el machete. 4.1.2.1 Gráfico de Puntos A continuación se muestra un gráfico de varias capas de construcción, con su correspondiente porción de código. En el mismo se buscará visualizar, a partir de la base de datos iris la relación entre el ancho y el largo de los petalos, mediante un gráfico de puntos. library(ggplot2) # cargamos la librería ggplot(data = iris, aes(x = Petal.Length, Petal.Width, color = Species))+ geom_point(alpha=0.75)+ labs(title = &quot;Medidas de los pétalos por especie&quot;)+ theme(legend.position = &#39;none&#39;)+ facet_wrap(~Species) 4.1.2.2 Capas del Gráfico Veamos ahora, el “paso a paso” del armado del mismo. En primera instancia solo defino los ejes. Y en este caso un color particular para cada Especie. g &lt;- ggplot(data = iris, aes(x = Petal.Length, Petal.Width, color = Species)) g Luego, defino el tipo de gráfico. El alpha me permite definir la intensidad de los puntos g &lt;- g + geom_point(alpha=0.25) g Las siguientes tres capas me permiten respectivamente: Definir el título del gráfico Quitar la leyenda Abrir el gráfico en tres fragmentos, uno para cada especie g &lt;- g + labs(title = &quot;Medidas de los pétalos por especie&quot;)+ theme(legend.position = &#39;none&#39;)+ facet_wrap(~Species) g 4.1.2.3 Extensiones de GGplot. La librería GGplot tiene a su vez muchas otras librerías que extienden sus potencialidades. Entre nuestras favoritas están: gganimate: Para hacer gráficos animados. ggridge: Para hacer gráficos de densidad faceteados ggally: Para hacer varios gráficos juntos. ^ library(GGally) ggpairs(iris, mapping = aes(color = Species)) library(ggridges) ggplot(iris, aes(x = Sepal.Length, y = Species, fill=Species)) + geom_density_ridges() También hay extensiones que te ayudan a escribir el código, como esquisse iris &lt;- iris #Correr en la consola esquisse::esquisser() 4.1.3 Dimensiones del gráfico Esta forma de pensar los gráficos nos permite repenser los distintos atributos como potenciales aliados a la hora de mostrar información multidimensional. Por ejemplo: color color = rellenofill = forma shape = tamaño size = transparencia alpha = Abrir un mismo gráfico según alguna variable discreta: facet_wrap() Los atributos que queremos que mapeen una variable, deben ir dentro del aes(), aes(... color = variable) Cuando queremos simplemente mejorar el diseño (es fijo), se asigna por fuera, o dentro de cada tipo de gráficos, geom_col(color = 'green'). 4.1.4 Treemaps library(treemapify) Trabajo doméstico no remunerado trabajo_no_remunerado &lt;- read_csv(&#39;../fuentes/prom_t_simul_dom_16_sexo__annio__g_edad_limpio.csv&#39;) trabajo_no_remunerado %&gt;% filter(sexo != &#39;TOTAL&#39;, grupo_edad != &#39;TOTAL&#39;) %&gt;% mutate(promedio_hs_diarias = as.numeric(promedio_hs_diarias), sexo = case_when(sexo==&#39;m&#39;~&#39;Mujer&#39;, sexo==&#39;v&#39;~&#39;Varón&#39;)) %&gt;% ggplot(., aes(area = promedio_hs_diarias, fill = promedio_hs_diarias, label = grupo_edad, subgroup = sexo)) + geom_treemap() + geom_treemap_subgroup_border() + geom_treemap_subgroup_text(place = &quot;centre&quot;, grow = T, alpha = 0.5, colour = &quot;black&quot;, fontface = &quot;italic&quot;, min.size = 0) + geom_treemap_text(colour = &quot;white&quot;, place = &quot;topleft&quot;, reflow = T)+ theme(legend.position = &#39;none&#39;) trabajo_no_remunerado %&gt;% filter(sexo != &#39;TOTAL&#39;, grupo_edad != &#39;TOTAL&#39;) %&gt;% mutate(promedio_hs_diarias = as.numeric(promedio_hs_diarias)) %&gt;% ggplot(., aes(area=promedio_hs_diarias, fill=sexo, label=sexo))+ geom_treemap() + geom_treemap_text(colour = &quot;white&quot;, place = &quot;topleft&quot;, reflow = T)+ facet_wrap(.~grupo_edad, ncol = 1) "],
["practica-guiada-2.html", "4.2 Práctica Guiada", " 4.2 Práctica Guiada 4.2.1 Graficos Ingresos - EPH Para esta práctica utilizaremos las variables de ingresos captadas por la Encuesta Permanente de Hogares A continuación utilzaremos los conceptos abordados, para realizar gráficos a partir de las variables de ingresos. #Cargamos las librerías a utilizar library(tidyverse) # tiene ggplot, dplyr, tidyr, y otros library(ggthemes) # estilos de gráficos library(ggrepel) # etiquetas de texto más prolijas que las de ggplot Individual_t119 &lt;- read.table(&quot;../fuentes/usu_individual_t119.txt&quot;, sep=&quot;;&quot;, dec=&quot;,&quot;, header = TRUE, fill = TRUE) 4.2.1.1 Boxplot de ingresos de la ocupación principal, según nivel educativo Hacemos un procesamiento simple: Sacamos los ingresos iguales a cero y las no respuestas de nivel educativo. Es importante que las variables sean del tipo que conceptualmente les corresponde (el nivel educativo es una variable categórica, no continua), para que el ggplot pueda graficarlo correctamente. # Las variables sexo( CH04 ) y Nivel educativo están codificadas como números, y el R las entiende como numéricas. class(Individual_t119$NIVEL_ED) ## [1] &quot;integer&quot; class(Individual_t119$CH04) ## [1] &quot;integer&quot; ggdata &lt;- Individual_t119 %&gt;% filter(P21&gt;0, !is.na(NIVEL_ED)) %&gt;% mutate(NIVEL_ED = as.factor(NIVEL_ED), CH04 = as.factor(CH04)) ggplot(ggdata, aes(x = NIVEL_ED, y = P21)) + geom_boxplot()+ scale_y_continuous(limits = c(0, 40000))#Restrinjo el gráfico hasta ingresos de $40000 Si queremos agregar la dimensión sexo, podemos hacer un facet_wrap() ggplot(ggdata, aes(x= NIVEL_ED, y = P21, group = NIVEL_ED, fill = NIVEL_ED )) + geom_boxplot()+ scale_y_continuous(limits = c(0, 40000))+ facet_wrap(~ CH04, labeller = &quot;label_both&quot;) Por la forma en que está presentado el gráfico, el foco de atención sigue puesto en las diferencias de ingresos entre niveles educativo. Simplemente se agrega un corte por la variable de sexo. Si lo que queremos hacer es poner el foco de atención en las diferencias por sexo, simplemente basta con invertir la variable x especificada con la variable utilizada en el facet_wrap ggplot(ggdata, aes(x= CH04, y = P21, group = CH04, fill = CH04 )) + geom_boxplot()+ scale_y_continuous(limits = c(0, 40000))+ facet_grid(~ NIVEL_ED, labeller = &quot;label_both&quot;) + theme(legend.position = &quot;none&quot;) 4.2.2 Histogramas Por ejemplo, si observamos el ingreso de la ocupación principal: hist_data &lt;-Individual_t119 %&gt;% filter(P21&gt;0) ggplot(hist_data, aes(x = P21,weights = PONDIIO))+ geom_histogram()+ scale_x_continuous(limits = c(0,50000)) En este gráfico, los posibles valores de p21 se dividen en 30 bins consecutivos y el gráfico muestra cuantas observaciones caen en cada uno de ellos 4.2.3 Kernels La función geom_density() nos permite construir kernels de la distribución. Es particularmente útil cuando tenemos una variable continua, dado que los histogramas rompen esa sensación de continuidad. Veamos un ejemplo sencillo con los ingresos de la ocupación principal. Luego iremos complejizandolo kernel_data &lt;-Individual_t119 %&gt;% filter(P21&gt;0) ggplot(kernel_data, aes(x = P21,weights = PONDIIO))+ geom_density()+ scale_x_continuous(limits = c(0,50000)) El eje y no tiene demasiada interpretabilidad en los Kernel, porque hace a la forma en que se construyen las distribuciones. El parametro adjust, dentro de la función geom_densitynos permite reducir o ampliar el rango de suavizado de la distribución. Su valor por default es 1. Veamos que sucede si lo seteamos en 2 ggplot(kernel_data, aes(x = P21,weights = PONDIIO))+ geom_density(adjust = 2)+ scale_x_continuous(limits = c(0,50000)) Como es esperable, la distribución del ingreso tiene “picos” en los valores redondos, ya que la gente suele declarar un valor aproximado al ingreso efectivo que percibe. Nadie declara ingresos de 30001. Al suavizar la serie con un kernel, eliminamos ese efecto.Si seteamos el rango para el suavizado en valores menores a 1, podemos observar estos picos. ggplot(kernel_data, aes(x = P21,weights = PONDIIO))+ geom_density(adjust = 0.01)+ scale_x_continuous(limits = c(0,50000)) Ahora bien, como en todo grafico de R, podemos seguir agregando dimensiones para enriquecer el análisis. kernel_data_2 &lt;- kernel_data %&gt;% mutate(CH04= case_when(CH04 == 1 ~ &quot;Varon&quot;, CH04 == 2 ~ &quot;Mujer&quot;)) ggplot(kernel_data_2, aes(x = P21, weights = PONDIIO, group = CH04, fill = CH04)) + geom_density(alpha=0.7,adjust =2)+ labs(x=&quot;Distribución del ingreso&quot;, y=&quot;&quot;, title=&quot; Total según tipo de ingreso y sexo&quot;, caption = &quot;Fuente: Encuesta Permanente de Hogares&quot;)+ scale_x_continuous(limits = c(0,50000))+ theme_tufte()+ scale_fill_gdocs()+ theme(legend.position = &quot;bottom&quot;, plot.title = element_text(size=12)) ggsave(filename = &quot;../resultados/Kernel_1.png&quot;,scale = 2) Podemos agregar aún la dimensión de ingreso laboral respecto del no laboral kernel_data_3 &lt;-kernel_data_2 %&gt;% select(REGION,P47T,T_VI, TOT_P12, P21 , PONDII, CH04) %&gt;% filter(!is.na(P47T), P47T &gt; 0 ) %&gt;% mutate(ingreso_laboral = TOT_P12 + P21, ingreso_no_laboral = T_VI) %&gt;% gather(., key = Tipo_ingreso, Ingreso, c((ncol(.)-1):ncol(.))) %&gt;% filter( Ingreso !=0)# Para este gráfico, quiero eliminar los ingresos = 0 kernel_data_3 %&gt;% sample_n(10) ## REGION P47T T_VI TOT_P12 P21 PONDII CH04 Tipo_ingreso Ingreso ## 1 44 20000 0 0 20000 180 Mujer ingreso_laboral 20000 ## 2 40 7400 5600 0 1800 404 Varon ingreso_no_laboral 5600 ## 3 42 10000 0 0 10000 96 Mujer ingreso_laboral 10000 ## 4 1 41000 8000 0 33000 3451 Mujer ingreso_no_laboral 8000 ## 5 40 27000 0 0 27000 208 Mujer ingreso_laboral 27000 ## 6 43 45000 0 0 30000 799 Mujer ingreso_laboral 30000 ## 7 1 11500 9500 0 2000 874 Mujer ingreso_no_laboral 9500 ## 8 42 35000 0 0 35000 618 Varon ingreso_laboral 35000 ## 9 40 30000 0 0 20000 344 Mujer ingreso_laboral 20000 ## 10 41 8500 0 0 8500 328 Mujer ingreso_laboral 8500 ggplot(kernel_data_3, aes( x = Ingreso, weights = PONDII, group = Tipo_ingreso, fill = Tipo_ingreso)) + geom_density(alpha=0.7,adjust =2)+ labs(x=&quot;Distribución del ingreso&quot;, y=&quot;&quot;, title=&quot; Total según tipo de ingreso y sexo&quot;, caption = &quot;Fuente: Encuesta Permanente de Hogares&quot;)+ scale_x_continuous(limits = c(0,50000))+ theme_tufte()+ scale_fill_gdocs()+ theme(legend.position = &quot;bottom&quot;, plot.title = element_text(size=12))+ facet_wrap(~ CH04, scales = &quot;free&quot;) ggsave(filename = &quot;../resultados/Kernel_2.png&quot;,scale = 2) En este tipo de gráficos, importa mucho qué variable se utiliza para facetear y qué variable para agrupar, ya que la construcción de la distribución es diferente. ggplot(kernel_data_3, aes( x = Ingreso, weights = PONDII, group = CH04, fill = CH04)) + geom_density(alpha=0.7,adjust =2)+ labs(x=&quot;Distribución del ingreso&quot;, y=&quot;&quot;, title=&quot; Total según tipo de ingreso y sexo&quot;, caption = &quot;Fuente: Encuesta Permanente de Hogares&quot;)+ scale_x_continuous(limits = c(0,50000))+ theme_tufte()+ scale_fill_gdocs()+ theme(legend.position = &quot;bottom&quot;, plot.title = element_text(size=12))+ facet_wrap(~Tipo_ingreso, scales = &quot;free&quot;) ggsave(filename = &quot;../resultados/Kernel_3.png&quot;,scale = 2) "],
["documentacion.html", "Capítulo-5 Documentación", " Capítulo-5 Documentación Manejo de las extensiones del software “Rmarkdown” y “RNotebook” para elaborar documentos de trabajo, presentaciones interactivas e informes: Opciones para mostrar u ocultar código en los reportes Definición de tamaño, títulos y formato con el cual se despliegan los gráficos y tablas en el informe Caracteres especiales para incluir múltiples recursos en el texto del informe: Links a páginas web, notas al pie, enumeraciones, cambios en el formato de letra (tamaño, negrita, cursiva) Código embebido en el texto para automatización de reportes "],
["explicacion-3.html", "5.1 Explicación", " 5.1 Explicación "],
["practica-guiada-3.html", "5.2 Práctica Guiada", " 5.2 Práctica Guiada "],
["diseno-y-analisis-de-encuestas.html", "Capítulo-6 Diseño y análisis de encuestas", " Capítulo-6 Diseño y análisis de encuestas Introducción al diseño de encuestas Presentación de la Encuesta Permanente de Hogares Generación de estadísticos de resumen en muestras estratificadas Utilización de los ponderadores "],
["explicacion-4.html", "6.1 Explicación", " 6.1 Explicación "],
["practica-guiada-4.html", "6.2 Práctica Guiada", " 6.2 Práctica Guiada "],
["programacion-funcional.html", "Capítulo-7 Programacion Funcional", " Capítulo-7 Programacion Funcional El objetivo de esta clase es introducir a los alumnos en el uso de la programación funcional. Es decir, en la utilización de funciones y el uso de controles de flujo de la información para la organización de su código. Estructuras de código condicionales Loops Creación de funciones a medida del usuario Librería purrr para programación funcional "],
["explicacion-5.html", "7.1 Explicación", " 7.1 Explicación library(tidyverse) 7.1.1 Loops Un loop es una estructura de código que nos permite aplicar iterativamente un mismo conjunto de comandos, variando el valor de una variable. Por ejemplo: for(i in 1:10){ print(i^2) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 ## [1] 36 ## [1] 49 ## [1] 64 ## [1] 81 ## [1] 100 Esto se lee como : “Recorre cada uno de los valores (i) del vector numérico 1 a 10, y para cada uno de ellos imprimí el cuadrado (i^2)”. Uno puede especificar la palabra que desee que tomé cada uno de los valores que debe tomar. En el ejemplo anterior fue i, pero bien podría ser la “Valores” for(Valores in 1:10){ print(Valores^2) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 ## [1] 36 ## [1] 49 ## [1] 64 ## [1] 81 ## [1] 100 Un loop puede iterar sobre cualquier tipo de vector, independientemente de lo que contenga. Los loops son una estructura básica que existen en cualquier lenguaje de programación. En R no recomendamos abusar de ellos porque hacen que el código sea más lento. 7.1.2 Estructuras Condicionales Las estructuras condiconales nos permiten ejecutar una porción de código en caso de que cumplan una condición lógica 7.1.2.1 if Su funcionamiento es el siguiente: if(condicion){codigo a ejecutar si se cumple la condición} if( 2+2 == 4){ print(&quot;Menos Mal&quot;) } ## [1] &quot;Menos Mal&quot; if( 2+2 == 148.24){ print(&quot;R, tenemos un problema&quot;) } 7.1.2.2 ifelse La función if_else() sirve para crear o modificar dicotómicamente un objeto/variable/vector a partir del cumplimiento de una o más condiciones lógicas. Su funcionamiento es el siguiente: if_else(condicion,función a aplicar si se cumple la condición,función a aplicar si no se cumple la condición) if_else(2+2==4, true = &quot;Joya&quot;,false = &quot;Error&quot;) ## [1] &quot;Joya&quot; 7.1.3 Funciones La creación de funciones propias nos permite automatizar todas aquellas partes del código que se repiten mucho. Una vez diseñadas, funcionan igual que cualquier comando. Por ejemplo, podemos definir la suma de dos elementos como suma &lt;- function(valor1, valor2) { valor1+valor2 } suma(5,6) ## [1] 11 Obviamente las funciones no son sólo para variables numéricas. Por ejemplo, podemos pegar dos strings con una flecha en el medio funcion_prueba &lt;- function(parametro1,parametro2) { paste(parametro1, parametro2, sep = &quot; &lt;--&gt; &quot;) } funcion_prueba(parametro1 = &quot;A ver&quot;, parametro2 = &quot;Que pasa&quot;) ## [1] &quot;A ver &lt;--&gt; Que pasa&quot; También podemos asignar un valor por default para los parametros en caso de que el usuario no defina su valor al utilizar la función. Otra_funcion_prueba &lt;- function(parametro1 ,parametro2 = &quot;String default&quot;) { paste(parametro1, parametro2, sep = &quot; &lt;--&gt; &quot;) } Otra_funcion_prueba(parametro1 = &quot;Valor 1 &quot;) ## [1] &quot;Valor 1 &lt;--&gt; String default&quot; Las funciones que creamos nosotros permanecen en el ambiente de R temporariamente. Cuando removemos los objetos del ambiente, la función deja de existir. Por ende, debemos incorporarla en cada uno de los scripts en la cual la necesitemos. Una buena práctica, es incorporar nuestras funciones útiles al comienzo de cada script junto a la carga de las librerías. Vale mencionar que lo que ocurre en una función, queda en la función excepto que explícitamente pidamos que devuelva el resultado, con el comando print(). Las funciones siempre devuelven el último objeto que se crea en ellas, o si explicitamente se utiliza el comando return() 7.1.4 PURRR3 MAP es la forma tidy de hacer loops. Además de ser más prolijo el código, es mucho más eficiente. La función map toma un input, una función para aplicar, y alguna otra cosa (por ejemplo parametros que necesite la función) map(.x, .f, …) map(VECTOR_O_LIST_INPUT, FUNCTION_A_APLICAR, OTROS_OPCIONALES) Usamos map2 cuando tenemos que pasar dos input, que se aplican sobre una función: map2(.x, .y, .f, …) map2(INPUT_UNO, INPUT_DOS, FUNCTION_A_APLICAR, OTROS_OPCIONALES) Si tenemos más de dos… pmap(.l, .f, …) pmap(VECTOR_O_LIST_INPUT, FUNCTION_A_APLICAR, OTROS_OPCIONALES) Por ejemplo. Si queremos utilizar la función prueba sobre los datos del dataframe ABC_123 ABC_123 &lt;- data.frame(Letras = LETTERS[1:20],Num = 1:20) funcion_prueba ## function(parametro1,parametro2) { ## paste(parametro1, parametro2, sep = &quot; &lt;--&gt; &quot;) ## } Si el resultado que queremos es que junte cada fila, necesitamos pasarle dos parámetros: utilizamos map2() resultado &lt;- map2(ABC_123$Letras,ABC_123$Num,funcion_prueba) resultado[1:3] ## [[1]] ## [1] &quot;A &lt;--&gt; 1&quot; ## ## [[2]] ## [1] &quot;B &lt;--&gt; 2&quot; ## ## [[3]] ## [1] &quot;C &lt;--&gt; 3&quot; La salida de los map() es una lista, no un vector, por lo que si lo metemos dentro de un dataframe se vería así: ABC_123 %&gt;% mutate(resultado= map2(Letras,Num,funcion_prueba)) ## Letras Num resultado ## 1 A 1 A &lt;--&gt; 1 ## 2 B 2 B &lt;--&gt; 2 ## 3 C 3 C &lt;--&gt; 3 ## 4 D 4 D &lt;--&gt; 4 ## 5 E 5 E &lt;--&gt; 5 ## 6 F 6 F &lt;--&gt; 6 ## 7 G 7 G &lt;--&gt; 7 ## 8 H 8 H &lt;--&gt; 8 ## 9 I 9 I &lt;--&gt; 9 ## 10 J 10 J &lt;--&gt; 10 ## 11 K 11 K &lt;--&gt; 11 ## 12 L 12 L &lt;--&gt; 12 ## 13 M 13 M &lt;--&gt; 13 ## 14 N 14 N &lt;--&gt; 14 ## 15 O 15 O &lt;--&gt; 15 ## 16 P 16 P &lt;--&gt; 16 ## 17 Q 17 Q &lt;--&gt; 17 ## 18 R 18 R &lt;--&gt; 18 ## 19 S 19 S &lt;--&gt; 19 ## 20 T 20 T &lt;--&gt; 20 al ponerlo dentro del dataframe desarma la lista y guarda cada elemento por separado. La magia de eso es que podemos guardar cualquier cosa en el dataframe no sólo valores, sino también listas, funciones, dataframes, etc. Si queremos recuperar los valores originales en este caso podemos usar unlist() resultado[1:3] %&gt;% unlist() ## [1] &quot;A &lt;--&gt; 1&quot; &quot;B &lt;--&gt; 2&quot; &quot;C &lt;--&gt; 3&quot; ABC_123 %&gt;% mutate(resultado= unlist(map2(Letras,Num,funcion_prueba))) ## Letras Num resultado ## 1 A 1 A &lt;--&gt; 1 ## 2 B 2 B &lt;--&gt; 2 ## 3 C 3 C &lt;--&gt; 3 ## 4 D 4 D &lt;--&gt; 4 ## 5 E 5 E &lt;--&gt; 5 ## 6 F 6 F &lt;--&gt; 6 ## 7 G 7 G &lt;--&gt; 7 ## 8 H 8 H &lt;--&gt; 8 ## 9 I 9 I &lt;--&gt; 9 ## 10 J 10 J &lt;--&gt; 10 ## 11 K 11 K &lt;--&gt; 11 ## 12 L 12 L &lt;--&gt; 12 ## 13 M 13 M &lt;--&gt; 13 ## 14 N 14 N &lt;--&gt; 14 ## 15 O 15 O &lt;--&gt; 15 ## 16 P 16 P &lt;--&gt; 16 ## 17 Q 17 Q &lt;--&gt; 17 ## 18 R 18 R &lt;--&gt; 18 ## 19 S 19 S &lt;--&gt; 19 ## 20 T 20 T &lt;--&gt; 20 Si lo que queríamos era que la función nos haga todas las combinaciones de letras y número, entonces lo que necesitamos es pasarle el segúndo parametro como algo fijo, poniendolo después de la función. map(ABC_123$Letras,funcion_prueba,ABC_123$Num)[1:2] ## [[1]] ## [1] &quot;A &lt;--&gt; 1&quot; &quot;A &lt;--&gt; 2&quot; &quot;A &lt;--&gt; 3&quot; &quot;A &lt;--&gt; 4&quot; &quot;A &lt;--&gt; 5&quot; ## [6] &quot;A &lt;--&gt; 6&quot; &quot;A &lt;--&gt; 7&quot; &quot;A &lt;--&gt; 8&quot; &quot;A &lt;--&gt; 9&quot; &quot;A &lt;--&gt; 10&quot; ## [11] &quot;A &lt;--&gt; 11&quot; &quot;A &lt;--&gt; 12&quot; &quot;A &lt;--&gt; 13&quot; &quot;A &lt;--&gt; 14&quot; &quot;A &lt;--&gt; 15&quot; ## [16] &quot;A &lt;--&gt; 16&quot; &quot;A &lt;--&gt; 17&quot; &quot;A &lt;--&gt; 18&quot; &quot;A &lt;--&gt; 19&quot; &quot;A &lt;--&gt; 20&quot; ## ## [[2]] ## [1] &quot;B &lt;--&gt; 1&quot; &quot;B &lt;--&gt; 2&quot; &quot;B &lt;--&gt; 3&quot; &quot;B &lt;--&gt; 4&quot; &quot;B &lt;--&gt; 5&quot; ## [6] &quot;B &lt;--&gt; 6&quot; &quot;B &lt;--&gt; 7&quot; &quot;B &lt;--&gt; 8&quot; &quot;B &lt;--&gt; 9&quot; &quot;B &lt;--&gt; 10&quot; ## [11] &quot;B &lt;--&gt; 11&quot; &quot;B &lt;--&gt; 12&quot; &quot;B &lt;--&gt; 13&quot; &quot;B &lt;--&gt; 14&quot; &quot;B &lt;--&gt; 15&quot; ## [16] &quot;B &lt;--&gt; 16&quot; &quot;B &lt;--&gt; 17&quot; &quot;B &lt;--&gt; 18&quot; &quot;B &lt;--&gt; 19&quot; &quot;B &lt;--&gt; 20&quot; En este caso, el map itera sobre cada elemento de letras, y para cada elemento i hace funcion_prueba(i,ABC$Num) y guarda el resultado en la lista si lo queremos meter en el dataframe ABC_123 %&gt;% mutate(resultado= map(Letras,funcion_prueba,Num)) ## Letras Num ## 1 A 1 ## 2 B 2 ## 3 C 3 ## 4 D 4 ## 5 E 5 ## 6 F 6 ## 7 G 7 ## 8 H 8 ## 9 I 9 ## 10 J 10 ## 11 K 11 ## 12 L 12 ## 13 M 13 ## 14 N 14 ## 15 O 15 ## 16 P 16 ## 17 Q 17 ## 18 R 18 ## 19 S 19 ## 20 T 20 ## resultado ## 1 A &lt;--&gt; 1, A &lt;--&gt; 2, A &lt;--&gt; 3, A &lt;--&gt; 4, A &lt;--&gt; 5, A &lt;--&gt; 6, A &lt;--&gt; 7, A &lt;--&gt; 8, A &lt;--&gt; 9, A &lt;--&gt; 10, A &lt;--&gt; 11, A &lt;--&gt; 12, A &lt;--&gt; 13, A &lt;--&gt; 14, A &lt;--&gt; 15, A &lt;--&gt; 16, A &lt;--&gt; 17, A &lt;--&gt; 18, A &lt;--&gt; 19, A &lt;--&gt; 20 ## 2 B &lt;--&gt; 1, B &lt;--&gt; 2, B &lt;--&gt; 3, B &lt;--&gt; 4, B &lt;--&gt; 5, B &lt;--&gt; 6, B &lt;--&gt; 7, B &lt;--&gt; 8, B &lt;--&gt; 9, B &lt;--&gt; 10, B &lt;--&gt; 11, B &lt;--&gt; 12, B &lt;--&gt; 13, B &lt;--&gt; 14, B &lt;--&gt; 15, B &lt;--&gt; 16, B &lt;--&gt; 17, B &lt;--&gt; 18, B &lt;--&gt; 19, B &lt;--&gt; 20 ## 3 C &lt;--&gt; 1, C &lt;--&gt; 2, C &lt;--&gt; 3, C &lt;--&gt; 4, C &lt;--&gt; 5, C &lt;--&gt; 6, C &lt;--&gt; 7, C &lt;--&gt; 8, C &lt;--&gt; 9, C &lt;--&gt; 10, C &lt;--&gt; 11, C &lt;--&gt; 12, C &lt;--&gt; 13, C &lt;--&gt; 14, C &lt;--&gt; 15, C &lt;--&gt; 16, C &lt;--&gt; 17, C &lt;--&gt; 18, C &lt;--&gt; 19, C &lt;--&gt; 20 ## 4 D &lt;--&gt; 1, D &lt;--&gt; 2, D &lt;--&gt; 3, D &lt;--&gt; 4, D &lt;--&gt; 5, D &lt;--&gt; 6, D &lt;--&gt; 7, D &lt;--&gt; 8, D &lt;--&gt; 9, D &lt;--&gt; 10, D &lt;--&gt; 11, D &lt;--&gt; 12, D &lt;--&gt; 13, D &lt;--&gt; 14, D &lt;--&gt; 15, D &lt;--&gt; 16, D &lt;--&gt; 17, D &lt;--&gt; 18, D &lt;--&gt; 19, D &lt;--&gt; 20 ## 5 E &lt;--&gt; 1, E &lt;--&gt; 2, E &lt;--&gt; 3, E &lt;--&gt; 4, E &lt;--&gt; 5, E &lt;--&gt; 6, E &lt;--&gt; 7, E &lt;--&gt; 8, E &lt;--&gt; 9, E &lt;--&gt; 10, E &lt;--&gt; 11, E &lt;--&gt; 12, E &lt;--&gt; 13, E &lt;--&gt; 14, E &lt;--&gt; 15, E &lt;--&gt; 16, E &lt;--&gt; 17, E &lt;--&gt; 18, E &lt;--&gt; 19, E &lt;--&gt; 20 ## 6 F &lt;--&gt; 1, F &lt;--&gt; 2, F &lt;--&gt; 3, F &lt;--&gt; 4, F &lt;--&gt; 5, F &lt;--&gt; 6, F &lt;--&gt; 7, F &lt;--&gt; 8, F &lt;--&gt; 9, F &lt;--&gt; 10, F &lt;--&gt; 11, F &lt;--&gt; 12, F &lt;--&gt; 13, F &lt;--&gt; 14, F &lt;--&gt; 15, F &lt;--&gt; 16, F &lt;--&gt; 17, F &lt;--&gt; 18, F &lt;--&gt; 19, F &lt;--&gt; 20 ## 7 G &lt;--&gt; 1, G &lt;--&gt; 2, G &lt;--&gt; 3, G &lt;--&gt; 4, G &lt;--&gt; 5, G &lt;--&gt; 6, G &lt;--&gt; 7, G &lt;--&gt; 8, G &lt;--&gt; 9, G &lt;--&gt; 10, G &lt;--&gt; 11, G &lt;--&gt; 12, G &lt;--&gt; 13, G &lt;--&gt; 14, G &lt;--&gt; 15, G &lt;--&gt; 16, G &lt;--&gt; 17, G &lt;--&gt; 18, G &lt;--&gt; 19, G &lt;--&gt; 20 ## 8 H &lt;--&gt; 1, H &lt;--&gt; 2, H &lt;--&gt; 3, H &lt;--&gt; 4, H &lt;--&gt; 5, H &lt;--&gt; 6, H &lt;--&gt; 7, H &lt;--&gt; 8, H &lt;--&gt; 9, H &lt;--&gt; 10, H &lt;--&gt; 11, H &lt;--&gt; 12, H &lt;--&gt; 13, H &lt;--&gt; 14, H &lt;--&gt; 15, H &lt;--&gt; 16, H &lt;--&gt; 17, H &lt;--&gt; 18, H &lt;--&gt; 19, H &lt;--&gt; 20 ## 9 I &lt;--&gt; 1, I &lt;--&gt; 2, I &lt;--&gt; 3, I &lt;--&gt; 4, I &lt;--&gt; 5, I &lt;--&gt; 6, I &lt;--&gt; 7, I &lt;--&gt; 8, I &lt;--&gt; 9, I &lt;--&gt; 10, I &lt;--&gt; 11, I &lt;--&gt; 12, I &lt;--&gt; 13, I &lt;--&gt; 14, I &lt;--&gt; 15, I &lt;--&gt; 16, I &lt;--&gt; 17, I &lt;--&gt; 18, I &lt;--&gt; 19, I &lt;--&gt; 20 ## 10 J &lt;--&gt; 1, J &lt;--&gt; 2, J &lt;--&gt; 3, J &lt;--&gt; 4, J &lt;--&gt; 5, J &lt;--&gt; 6, J &lt;--&gt; 7, J &lt;--&gt; 8, J &lt;--&gt; 9, J &lt;--&gt; 10, J &lt;--&gt; 11, J &lt;--&gt; 12, J &lt;--&gt; 13, J &lt;--&gt; 14, J &lt;--&gt; 15, J &lt;--&gt; 16, J &lt;--&gt; 17, J &lt;--&gt; 18, J &lt;--&gt; 19, J &lt;--&gt; 20 ## 11 K &lt;--&gt; 1, K &lt;--&gt; 2, K &lt;--&gt; 3, K &lt;--&gt; 4, K &lt;--&gt; 5, K &lt;--&gt; 6, K &lt;--&gt; 7, K &lt;--&gt; 8, K &lt;--&gt; 9, K &lt;--&gt; 10, K &lt;--&gt; 11, K &lt;--&gt; 12, K &lt;--&gt; 13, K &lt;--&gt; 14, K &lt;--&gt; 15, K &lt;--&gt; 16, K &lt;--&gt; 17, K &lt;--&gt; 18, K &lt;--&gt; 19, K &lt;--&gt; 20 ## 12 L &lt;--&gt; 1, L &lt;--&gt; 2, L &lt;--&gt; 3, L &lt;--&gt; 4, L &lt;--&gt; 5, L &lt;--&gt; 6, L &lt;--&gt; 7, L &lt;--&gt; 8, L &lt;--&gt; 9, L &lt;--&gt; 10, L &lt;--&gt; 11, L &lt;--&gt; 12, L &lt;--&gt; 13, L &lt;--&gt; 14, L &lt;--&gt; 15, L &lt;--&gt; 16, L &lt;--&gt; 17, L &lt;--&gt; 18, L &lt;--&gt; 19, L &lt;--&gt; 20 ## 13 M &lt;--&gt; 1, M &lt;--&gt; 2, M &lt;--&gt; 3, M &lt;--&gt; 4, M &lt;--&gt; 5, M &lt;--&gt; 6, M &lt;--&gt; 7, M &lt;--&gt; 8, M &lt;--&gt; 9, M &lt;--&gt; 10, M &lt;--&gt; 11, M &lt;--&gt; 12, M &lt;--&gt; 13, M &lt;--&gt; 14, M &lt;--&gt; 15, M &lt;--&gt; 16, M &lt;--&gt; 17, M &lt;--&gt; 18, M &lt;--&gt; 19, M &lt;--&gt; 20 ## 14 N &lt;--&gt; 1, N &lt;--&gt; 2, N &lt;--&gt; 3, N &lt;--&gt; 4, N &lt;--&gt; 5, N &lt;--&gt; 6, N &lt;--&gt; 7, N &lt;--&gt; 8, N &lt;--&gt; 9, N &lt;--&gt; 10, N &lt;--&gt; 11, N &lt;--&gt; 12, N &lt;--&gt; 13, N &lt;--&gt; 14, N &lt;--&gt; 15, N &lt;--&gt; 16, N &lt;--&gt; 17, N &lt;--&gt; 18, N &lt;--&gt; 19, N &lt;--&gt; 20 ## 15 O &lt;--&gt; 1, O &lt;--&gt; 2, O &lt;--&gt; 3, O &lt;--&gt; 4, O &lt;--&gt; 5, O &lt;--&gt; 6, O &lt;--&gt; 7, O &lt;--&gt; 8, O &lt;--&gt; 9, O &lt;--&gt; 10, O &lt;--&gt; 11, O &lt;--&gt; 12, O &lt;--&gt; 13, O &lt;--&gt; 14, O &lt;--&gt; 15, O &lt;--&gt; 16, O &lt;--&gt; 17, O &lt;--&gt; 18, O &lt;--&gt; 19, O &lt;--&gt; 20 ## 16 P &lt;--&gt; 1, P &lt;--&gt; 2, P &lt;--&gt; 3, P &lt;--&gt; 4, P &lt;--&gt; 5, P &lt;--&gt; 6, P &lt;--&gt; 7, P &lt;--&gt; 8, P &lt;--&gt; 9, P &lt;--&gt; 10, P &lt;--&gt; 11, P &lt;--&gt; 12, P &lt;--&gt; 13, P &lt;--&gt; 14, P &lt;--&gt; 15, P &lt;--&gt; 16, P &lt;--&gt; 17, P &lt;--&gt; 18, P &lt;--&gt; 19, P &lt;--&gt; 20 ## 17 Q &lt;--&gt; 1, Q &lt;--&gt; 2, Q &lt;--&gt; 3, Q &lt;--&gt; 4, Q &lt;--&gt; 5, Q &lt;--&gt; 6, Q &lt;--&gt; 7, Q &lt;--&gt; 8, Q &lt;--&gt; 9, Q &lt;--&gt; 10, Q &lt;--&gt; 11, Q &lt;--&gt; 12, Q &lt;--&gt; 13, Q &lt;--&gt; 14, Q &lt;--&gt; 15, Q &lt;--&gt; 16, Q &lt;--&gt; 17, Q &lt;--&gt; 18, Q &lt;--&gt; 19, Q &lt;--&gt; 20 ## 18 R &lt;--&gt; 1, R &lt;--&gt; 2, R &lt;--&gt; 3, R &lt;--&gt; 4, R &lt;--&gt; 5, R &lt;--&gt; 6, R &lt;--&gt; 7, R &lt;--&gt; 8, R &lt;--&gt; 9, R &lt;--&gt; 10, R &lt;--&gt; 11, R &lt;--&gt; 12, R &lt;--&gt; 13, R &lt;--&gt; 14, R &lt;--&gt; 15, R &lt;--&gt; 16, R &lt;--&gt; 17, R &lt;--&gt; 18, R &lt;--&gt; 19, R &lt;--&gt; 20 ## 19 S &lt;--&gt; 1, S &lt;--&gt; 2, S &lt;--&gt; 3, S &lt;--&gt; 4, S &lt;--&gt; 5, S &lt;--&gt; 6, S &lt;--&gt; 7, S &lt;--&gt; 8, S &lt;--&gt; 9, S &lt;--&gt; 10, S &lt;--&gt; 11, S &lt;--&gt; 12, S &lt;--&gt; 13, S &lt;--&gt; 14, S &lt;--&gt; 15, S &lt;--&gt; 16, S &lt;--&gt; 17, S &lt;--&gt; 18, S &lt;--&gt; 19, S &lt;--&gt; 20 ## 20 T &lt;--&gt; 1, T &lt;--&gt; 2, T &lt;--&gt; 3, T &lt;--&gt; 4, T &lt;--&gt; 5, T &lt;--&gt; 6, T &lt;--&gt; 7, T &lt;--&gt; 8, T &lt;--&gt; 9, T &lt;--&gt; 10, T &lt;--&gt; 11, T &lt;--&gt; 12, T &lt;--&gt; 13, T &lt;--&gt; 14, T &lt;--&gt; 15, T &lt;--&gt; 16, T &lt;--&gt; 17, T &lt;--&gt; 18, T &lt;--&gt; 19, T &lt;--&gt; 20 Ahora cada fila tiene un vector de 20 elementos guardado en la columna resultado 7.1.5 Funciones implícitas no es necesario que definamos la función de antemano. Podemos usar funciones implícitas map_dbl(c(1:10), function(x) x^2) ## [1] 1 4 9 16 25 36 49 64 81 100 map2_dbl(c(1:10),c(11:20), function(x,y) x*y) ## [1] 11 24 39 56 75 96 119 144 171 200 7.1.6 Funciones lambda incluso más conciso que las funciones implíictas son las funciones lambda donde definimos las variables como .x .y, etc. La flexibilidad de estas expresiones es limitada, pero puede ser útil en algunos casos. map_dbl(c(1:10),~.x^2) ## [1] 1 4 9 16 25 36 49 64 81 100 map2_dbl(c(1:10),c(11:20),~.x*.y) ## [1] 11 24 39 56 75 96 119 144 171 200 7.1.7 Walk Las funciones Walk Tienen la misma forma que los map, pero se usan cuando lo que queremos iterar no genera una salida, sino que nos interesan los efectos secundarios que generan. map2(ABC_123$Letras,ABC_123$Num,funcion_prueba)[1:3] ## [[1]] ## [1] &quot;A &lt;--&gt; 1&quot; ## ## [[2]] ## [1] &quot;B &lt;--&gt; 2&quot; ## ## [[3]] ## [1] &quot;C &lt;--&gt; 3&quot; walk2(ABC_123$Letras,ABC_123$Num,funcion_prueba) imprimir_salida &lt;- function(x,y){ print(funcion_prueba(x,y)) } walk2(ABC_123$Letras,ABC_123$Num,imprimir_salida) ## [1] &quot;A &lt;--&gt; 1&quot; ## [1] &quot;B &lt;--&gt; 2&quot; ## [1] &quot;C &lt;--&gt; 3&quot; ## [1] &quot;D &lt;--&gt; 4&quot; ## [1] &quot;E &lt;--&gt; 5&quot; ## [1] &quot;F &lt;--&gt; 6&quot; ## [1] &quot;G &lt;--&gt; 7&quot; ## [1] &quot;H &lt;--&gt; 8&quot; ## [1] &quot;I &lt;--&gt; 9&quot; ## [1] &quot;J &lt;--&gt; 10&quot; ## [1] &quot;K &lt;--&gt; 11&quot; ## [1] &quot;L &lt;--&gt; 12&quot; ## [1] &quot;M &lt;--&gt; 13&quot; ## [1] &quot;N &lt;--&gt; 14&quot; ## [1] &quot;O &lt;--&gt; 15&quot; ## [1] &quot;P &lt;--&gt; 16&quot; ## [1] &quot;Q &lt;--&gt; 17&quot; ## [1] &quot;R &lt;--&gt; 18&quot; ## [1] &quot;S &lt;--&gt; 19&quot; ## [1] &quot;T &lt;--&gt; 20&quot; Eso que vemos es el efecto secundario dentro de la función (imprimir) 7.1.8 Cuando usar estas herramientas? A lo largo del curso vimos diferentes técnicas para manipulación de datos. En particular, la librería dplyr nos permitía fácilmente modificar y crear nuevas variables, agrupando. Cuando usamos dplyr y cuando usamos purrr. Si trabajamos sobre un DF simple, sin variables anidadas (lo que conocíamos hasta hoy) podemos usar dplyr Si queremos trabajar con DF anidados, con cosas que no son DF, o si el resultado de la operación que vamos a realizar a nivel file es algo distinto a un valor único, nos conviene usar map y purrr Las funciones walk son útiles por ejemplo para escribir archivos en disco de forma iterativa. Algo que no genera una salida basado en https://jennybc.github.io/purrr-tutorial/ls03_map-function-syntax.html↩ "],
["practica-guiada-5.html", "7.2 Práctica Guiada", " 7.2 Práctica Guiada library(fs) library(tidyverse) library(openxlsx) library(glue) 7.2.1 Ejemplo 1: Iterando en la EPH Lo primero que necesitamos es definir un vector o lista sobre el que iterar. Por ejemplo, podemos armar un vector con los path a las bases individuales, con el comando fs::dir_ls bases_individuales_path &lt;- dir_ls(path = &#39;../fuentes/&#39;, regexp= &#39;individual&#39;) bases_individuales_path ## ../fuentes/usu_individual_t119.txt ../fuentes/usu_individual_t418.txt Luego, como en la función que usamos para leer las bases definimos muchos parametros, nos podemos armar una función wrapper que sólo necesite un parámetro, y que simplifique la escritura del map leer_base_eph &lt;- function(path) { read.table(path,sep=&quot;;&quot;, dec=&quot;,&quot;, header = TRUE, fill = TRUE) %&gt;% select(ANO4,TRIMESTRE,REGION,P21,CH04, CH06) } bases_df &lt;- tibble(bases_individuales_path) %&gt;% mutate(base = map(bases_individuales_path, leer_base_eph)) bases_df ## # A tibble: 2 x 2 ## bases_individuales_path base ## &lt;fs::path&gt; &lt;list&gt; ## 1 ../fuentes/usu_individual_t119.txt &lt;df[,6] [59,369 × 6]&gt; ## 2 ../fuentes/usu_individual_t418.txt &lt;df[,6] [57,418 × 6]&gt; El resultado es un DF donde la columna base tiene en cada fila, otro DF con la base de la EPH de ese período. Esto es lo que llamamos un nested DF o dataframe nesteado pa les pibes. Si queremos juntar todo, podemos usar unnest() bases_df &lt;- bases_df %&gt;% unnest() bases_df ## # A tibble: 116,787 x 7 ## bases_individuales_path ANO4 TRIMESTRE REGION P21 CH04 ## &lt;fs::path&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 ../fuentes/usu_individual_t119.txt 2019 1 41 0 2 ## 2 ../fuentes/usu_individual_t119.txt 2019 1 41 0 2 ## 3 ../fuentes/usu_individual_t119.txt 2019 1 41 0 1 ## 4 ../fuentes/usu_individual_t119.txt 2019 1 41 5000 2 ## 5 ../fuentes/usu_individual_t119.txt 2019 1 41 0 2 ## 6 ../fuentes/usu_individual_t119.txt 2019 1 41 8000 1 ## 7 ../fuentes/usu_individual_t119.txt 2019 1 41 0 1 ## 8 ../fuentes/usu_individual_t119.txt 2019 1 41 0 2 ## 9 ../fuentes/usu_individual_t119.txt 2019 1 41 0 2 ## 10 ../fuentes/usu_individual_t119.txt 2019 1 41 3000 1 ## # … with 116,777 more rows, and 1 more variable: CH06 &lt;int&gt; ¿Qué pasa si los DF que tenemos nesteados no tienen la misma cantidad de columnas? Esto mismo lo podemos usar para fragmentar el datastet por alguna variable, con el group_by() bases_df %&gt;% group_by(REGION) %&gt;% nest() ## # A tibble: 6 x 2 ## REGION data ## &lt;int&gt; &lt;list&gt; ## 1 41 &lt;tibble [11,509 × 6]&gt; ## 2 44 &lt;tibble [14,204 × 6]&gt; ## 3 42 &lt;tibble [11,150 × 6]&gt; ## 4 43 &lt;tibble [34,702 × 6]&gt; ## 5 40 &lt;tibble [24,432 × 6]&gt; ## 6 1 &lt;tibble [20,790 × 6]&gt; Así, para cada región tenemos un DF. ¿ De qué sirve todo esto? No todo en la vida es un Dataframe. Hay estucturas de datos que no se pueden normalizar a filas y columnas. En esos casos recurríamos tradicionalmente a los loops. Con MAP podemos tener los elementos agrupados en un sólo objeto y aún conservar sus formas diferentes. 7.2.2 Ejemplo 2. Regresión lineal Si bien no nos vamos a meter en el detalle del modelo lineal hoy, es útil usarlo como ejemplo de lo que podemos hacer con MAP. Planteamos el modelo \\[ P21 = \\beta_0 + \\beta_1*CH04 + \\beta_2*CH06 \\] Osea, un modleo que explica el ingreso según sexo y edad lmfit &lt;- lm(P21~factor(CH04)+CH06,data = bases_df) summary(lmfit) ## ## Call: ## lm(formula = P21 ~ factor(CH04) + CH06, data = bases_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15472 -6606 -3367 2148 590198 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4853.196 74.509 65.14 &lt;2e-16 *** ## factor(CH04)2 -4063.112 72.200 -56.27 &lt;2e-16 *** ## CH06 103.095 1.612 63.97 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12300 on 116784 degrees of freedom ## Multiple R-squared: 0.05511, Adjusted R-squared: 0.0551 ## F-statistic: 3406 on 2 and 116784 DF, p-value: &lt; 2.2e-16 (al final de la clase podemos charlar sobre los resultados, si hay interés :-) ) De forma Tidy, la librería broom nos da los resultados en un DF. broom::tidy(lmfit) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 4853. 74.5 65.1 0 ## 2 factor(CH04)2 -4063. 72.2 -56.3 0 ## 3 CH06 103. 1.61 64.0 0 Si lo queremos hacer por region 7.2.2.1 Loopeando resultados &lt;- tibble() for (region in unique(bases_df$REGION)) { data &lt;- bases_df %&gt;% filter(REGION==region) lmfit &lt;- lm(P21~factor(CH04)+CH06,data = data) lmtidy &lt;- broom::tidy(lmfit) lmtidy$region &lt;- region resultados &lt;- bind_rows(resultados,lmtidy) } resultados ## # A tibble: 18 x 6 ## term estimate std.error statistic p.value region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 (Intercept) 3768. 185. 20.3 3.15e- 90 41 ## 2 factor(CH04)2 -3814. 180. -21.2 6.00e- 98 41 ## 3 CH06 106. 4.18 25.3 1.12e-137 41 ## 4 (Intercept) 7156. 291. 24.6 1.09e-130 44 ## 5 factor(CH04)2 -5938. 278. -21.4 1.42e- 99 44 ## 6 CH06 145. 6.32 23.0 1.40e-114 44 ## 7 (Intercept) 4930. 231. 21.4 2.15e- 99 42 ## 8 factor(CH04)2 -4007. 224. -17.9 1.71e- 70 42 ## 9 CH06 97.8 4.95 19.7 2.68e- 85 42 ## 10 (Intercept) 5107. 131. 39.0 0. 43 ## 11 factor(CH04)2 -3949. 127. -31.1 5.02e-209 43 ## 12 CH06 83.5 2.78 30.0 3.87e-195 43 ## 13 (Intercept) 3329. 128. 26.0 4.12e-147 40 ## 14 factor(CH04)2 -3239. 125. -25.9 3.74e-146 40 ## 15 CH06 122. 2.89 42.2 0. 40 ## 16 (Intercept) 5196. 197. 26.4 3.45e-151 1 ## 17 factor(CH04)2 -4051. 189. -21.4 1.80e-100 1 ## 18 CH06 88.2 4.12 21.4 1.98e-100 1 7.2.2.2 Usando MAP Primero me armo una funcion que me simplifica el codigo fun&lt;-function(porcion,grupo) { broom::tidy(lm(P21~factor(CH04)+CH06,data = porcion))} bases_df_lm &lt;- bases_df %&gt;% group_by(REGION) %&gt;% nest() %&gt;% mutate(lm = map(data,fun)) bases_df_lm ## # A tibble: 6 x 3 ## REGION data lm ## &lt;int&gt; &lt;list&gt; &lt;list&gt; ## 1 41 &lt;tibble [11,509 × 6]&gt; &lt;tibble [3 × 5]&gt; ## 2 44 &lt;tibble [14,204 × 6]&gt; &lt;tibble [3 × 5]&gt; ## 3 42 &lt;tibble [11,150 × 6]&gt; &lt;tibble [3 × 5]&gt; ## 4 43 &lt;tibble [34,702 × 6]&gt; &lt;tibble [3 × 5]&gt; ## 5 40 &lt;tibble [24,432 × 6]&gt; &lt;tibble [3 × 5]&gt; ## 6 1 &lt;tibble [20,790 × 6]&gt; &lt;tibble [3 × 5]&gt; bases_df_lm %&gt;% unnest(lm) ## # A tibble: 18 x 6 ## REGION term estimate std.error statistic p.value ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 41 (Intercept) 3768. 185. 20.3 3.15e- 90 ## 2 41 factor(CH04)2 -3814. 180. -21.2 6.00e- 98 ## 3 41 CH06 106. 4.18 25.3 1.12e-137 ## 4 44 (Intercept) 7156. 291. 24.6 1.09e-130 ## 5 44 factor(CH04)2 -5938. 278. -21.4 1.42e- 99 ## 6 44 CH06 145. 6.32 23.0 1.40e-114 ## 7 42 (Intercept) 4930. 231. 21.4 2.15e- 99 ## 8 42 factor(CH04)2 -4007. 224. -17.9 1.71e- 70 ## 9 42 CH06 97.8 4.95 19.7 2.68e- 85 ## 10 43 (Intercept) 5107. 131. 39.0 0. ## 11 43 factor(CH04)2 -3949. 127. -31.1 5.02e-209 ## 12 43 CH06 83.5 2.78 30.0 3.87e-195 ## 13 40 (Intercept) 3329. 128. 26.0 4.12e-147 ## 14 40 factor(CH04)2 -3239. 125. -25.9 3.74e-146 ## 15 40 CH06 122. 2.89 42.2 0. ## 16 1 (Intercept) 5196. 197. 26.4 3.45e-151 ## 17 1 factor(CH04)2 -4051. 189. -21.4 1.80e-100 ## 18 1 CH06 88.2 4.12 21.4 1.98e-100 O incluso más facil, utilizando group_modify (que es un atajo que solo acepta DF) bases_df %&gt;% group_by(REGION) %&gt;% group_modify(fun) ## # A tibble: 18 x 6 ## # Groups: REGION [6] ## REGION term estimate std.error statistic p.value ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 (Intercept) 5196. 197. 26.4 3.45e-151 ## 2 1 factor(CH04)2 -4051. 189. -21.4 1.80e-100 ## 3 1 CH06 88.2 4.12 21.4 1.98e-100 ## 4 40 (Intercept) 3329. 128. 26.0 4.12e-147 ## 5 40 factor(CH04)2 -3239. 125. -25.9 3.74e-146 ## 6 40 CH06 122. 2.89 42.2 0. ## 7 41 (Intercept) 3768. 185. 20.3 3.15e- 90 ## 8 41 factor(CH04)2 -3814. 180. -21.2 6.00e- 98 ## 9 41 CH06 106. 4.18 25.3 1.12e-137 ## 10 42 (Intercept) 4930. 231. 21.4 2.15e- 99 ## 11 42 factor(CH04)2 -4007. 224. -17.9 1.71e- 70 ## 12 42 CH06 97.8 4.95 19.7 2.68e- 85 ## 13 43 (Intercept) 5107. 131. 39.0 0. ## 14 43 factor(CH04)2 -3949. 127. -31.1 5.02e-209 ## 15 43 CH06 83.5 2.78 30.0 3.87e-195 ## 16 44 (Intercept) 7156. 291. 24.6 1.09e-130 ## 17 44 factor(CH04)2 -5938. 278. -21.4 1.42e- 99 ## 18 44 CH06 145. 6.32 23.0 1.40e-114 Pero MAP sirve para operar con cualquier objeto de R. Por ejemplo podemos guardar el objeto S3:lm que es la regresion lineal entrenada. Ese objeto no es ni un vector, ni una lista, ni un DF. No es una estructura de datos, sino que es algo distinto, con propiedades como predict() para predecir, el summary() que vimos, etc. fun&lt;-function(porcion,grupo) { lm(P21~factor(CH04)+CH06,data = porcion)} bases_df %&gt;% group_by(REGION) %&gt;% nest() %&gt;% mutate(lm = map(data,fun)) ## # A tibble: 6 x 3 ## REGION data lm ## &lt;int&gt; &lt;list&gt; &lt;list&gt; ## 1 41 &lt;tibble [11,509 × 6]&gt; &lt;lm&gt; ## 2 44 &lt;tibble [14,204 × 6]&gt; &lt;lm&gt; ## 3 42 &lt;tibble [11,150 × 6]&gt; &lt;lm&gt; ## 4 43 &lt;tibble [34,702 × 6]&gt; &lt;lm&gt; ## 5 40 &lt;tibble [24,432 × 6]&gt; &lt;lm&gt; ## 6 1 &lt;tibble [20,790 × 6]&gt; &lt;lm&gt; 7.2.3 Ejemplo 3: Gráficos en serie Veamos un tercer ejemplo con otra base de datos que ya conocemos: Gapminder, que muestra algunos datos sobre la población de los países por año. El objetivo de este ejercicio es hacer un gráfico por país de forma automática. Primero veamos los datos library(gapminder) gapminder_unfiltered %&gt;% sample_n(10) ## # A tibble: 10 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Norway Europe 1964 73.6 3694339 14440. ## 2 Togo Africa 1962 43.9 1528098 1068. ## 3 Austria Europe 1970 70.1 7467086 15079. ## 4 Congo, Dem. Rep. Africa 1957 40.7 15577932 906. ## 5 Qatar Asia 1972 62.1 131794 81069. ## 6 Bulgaria Europe 1986 71.6 8958770 8236. ## 7 Poland Europe 1963 68.6 30662122 5597. ## 8 Zimbabwe Africa 1957 50.5 3646340 519. ## 9 Uganda Africa 2007 51.5 29170398 1056. ## 10 Mongolia Asia 2007 66.8 2874127 3096. la base tiene la siguiente info: country: Nombre del país continent: Nombre del continente year: año lifeExp: Esperanza de vida al nacer pop: Población gdpPercap Vamos a hacer un gráfico sencillo para Argentina data_argentina &lt;- gapminder_unfiltered %&gt;% filter(country==&#39;Argentina&#39;) ggplot(data_argentina, aes(year, lifeExp, size= pop, color=gdpPercap))+ geom_point()+ geom_line(alpha=0.6)+ labs(title = unique(data_argentina$country)) Ahora que tenemos una idea de lo que queremos gráficar lo podemos poner adentro de una función que grafique. # definimos la función graficar_pais &lt;- function(data, pais){ ggplot(data, aes(year, lifeExp, size= pop, color=gdpPercap))+ geom_point()+ geom_line(alpha=0.6)+ labs(title = pais) } probamos la función para un caso graficar_pais(data_argentina, &#39;Argentina&#39;) Nos armamos un dataset nesteado gapminder_nest &lt;- gapminder_unfiltered %&gt;% group_by(country) %&gt;% nest() gapminder_nest %&gt;% sample_n(10) ## # A tibble: 10 x 2 ## country data ## &lt;fct&gt; &lt;list&gt; ## 1 Puerto Rico &lt;tibble [13 × 5]&gt; ## 2 Korea, Rep. &lt;tibble [12 × 5]&gt; ## 3 Samoa &lt;tibble [7 × 5]&gt; ## 4 Afghanistan &lt;tibble [12 × 5]&gt; ## 5 Malaysia &lt;tibble [12 × 5]&gt; ## 6 Italy &lt;tibble [56 × 5]&gt; ## 7 French Polynesia &lt;tibble [9 × 5]&gt; ## 8 Slovenia &lt;tibble [32 × 5]&gt; ## 9 United Arab Emirates &lt;tibble [8 × 5]&gt; ## 10 Namibia &lt;tibble [12 × 5]&gt; Ahora podemos crear una nueva columna que contenga los gráficos gapminder_nest &lt;- gapminder_nest %&gt;% mutate(grafico= map2(.x = data, .y = country,.f = graficar_pais)) gapminder_nest %&gt;% sample_n(10) ## # A tibble: 10 x 3 ## country data grafico ## &lt;fct&gt; &lt;list&gt; &lt;list&gt; ## 1 Cyprus &lt;tibble [8 × 5]&gt; &lt;gg&gt; ## 2 Somalia &lt;tibble [12 × 5]&gt; &lt;gg&gt; ## 3 Poland &lt;tibble [52 × 5]&gt; &lt;gg&gt; ## 4 Uzbekistan &lt;tibble [4 × 5]&gt; &lt;gg&gt; ## 5 Ecuador &lt;tibble [12 × 5]&gt; &lt;gg&gt; ## 6 Sweden &lt;tibble [58 × 5]&gt; &lt;gg&gt; ## 7 Kenya &lt;tibble [12 × 5]&gt; &lt;gg&gt; ## 8 Ethiopia &lt;tibble [12 × 5]&gt; &lt;gg&gt; ## 9 Guinea-Bissau &lt;tibble [12 × 5]&gt; &lt;gg&gt; ## 10 Portugal &lt;tibble [58 × 5]&gt; &lt;gg&gt; Veamos un ejemplo gapminder_nest$grafico[2] ## [[1]] Ahora podemos guardar todos los gráficos en un archivo PDF pdf(&#39;../resultados/graficos_gapminder.pdf&#39;) gapminder_nest$grafico dev.off() "],
["modelo-lineal.html", "Capítulo-8 Modelo Lineal ", " Capítulo-8 Modelo Lineal "],
["explicacion-6.html", "8.1 Explicación", " 8.1 Explicación En este módulo vamos a ver cómo analizar la relación entre dos variables. Primero, veremos los conceptos de covarianza y correlación, y luego avanzaremos hasta el modelo lineal. knitr::opts_chunk$set(warning = FALSE, message = FALSE) library(tidyverse) library(modelr) library(GGally) library(plot3D) 8.1.1 Covarianza y Correlación. La covarianza mide cómo varían de forma conjunta dos variables, en promedio. Se define como: \\[ \\text{cov}(x,y)=\\frac{1}{n}\\sum_{i=1}^n(x_i-\\bar x)(y_i-\\bar y) \\] Esto es: La covarianza entre dos variables, \\(x\\) e \\(y\\) es el promedio (noten que hay una sumatoria y un dividido n) de las diferencias de los puntos a sus medias en \\(x\\) e \\(y\\). tratemos de entender el trabalenguas con la ayuda del siguiente gráfico: Aquí marcamos \\(\\bar x\\) y \\(\\bar y\\) y dividimos el gráfico en cuatro cuadrantes. En el primer cuadrante los puntos son más chicos a sus medias en \\(x\\) y en \\(y\\), \\((x-\\hat x)\\) es negativo y \\((y-\\hat y)\\) también. Por lo tanto, su producto es positivo. En el segundo cuadrante la diferencia es negativa en x, pero positiva en y. Por lo tanto el producto es negativo. En el tercer cuadrante la diferencia es negativa en y, pero positiva en x. Por lo tanto el producto es negativo. Finalmente, en el cuarto cuadrante las diferencias son positivas tanto en x como en y, y por lo tanto también el producto. Si la covarianza es positiva y grande, entonces valores chicos en una de las variables suceden en conjunto con valores chicos en la otra,y viceversa. Al contrario, si la covarianza es negativa y grande, entonces valores altos de una variable suceden en conjunto con valores pequeños de la otra y viceversa. La correlación se define como sigue: \\[\\rho_{x,y}=\\frac{cov(x,y)}{\\sigma_x \\sigma_y}\\] Es decir, normalizamos la covarianza por el desvío en \\(x\\) y en \\(y\\). de esta forma, la correlación se define entre -1 y 1. 8.1.1.1 ggpairs Para ver una implementación práctica de estos conceptos, vamos a utilizar la librería GGally para graficar la correlación por pares de variables. Con ggpairs(), podemos graficar todas las variables, y buscar las correlaciones. Coloreamos por: -\\(am\\): Tipo de transmisión: automático (am=0) o manual (am=1) mtcars %&gt;% select(-carb,-vs) %&gt;% mutate(cyl = factor(cyl), am = factor(am)) %&gt;% ggpairs(., title = &quot;Matriz de correlaciones&quot;, mapping = aes(colour= am)) Veamos la correlación entre: \\(mpg\\): Miles/(US) gallon. Eficiencia de combustible \\(hp\\): Gross horsepower: Potencia del motor cor(mtcars$mpg, mtcars$hp) ## [1] -0.7761684 nos da negativa y alta. Si quisiéramos testear la significatividad de este estimador, podemos realizar un test: \\(H_0\\) : ρ =0 \\(H_1\\) : ρ \\(\\neq\\) 0 cor.test(mtcars$mpg,mtcars$hp) ## ## Pearson&#39;s product-moment correlation ## ## data: mtcars$mpg and mtcars$hp ## t = -6.7424, df = 30, p-value = 1.788e-07 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.8852686 -0.5860994 ## sample estimates: ## cor ## -0.7761684 Con este p-value rechazamos \\(H_0\\) 8.1.2 Modelo Lineal sigamos utilizando los datos de sim1 ggplot(sim1, aes(x, y)) + geom_point() Se puede ver un patrón fuerte en los datos. Pareciera que el modelo lineal y = a_0 + a_1 * x podría servir. 8.1.2.1 Modelos al azar Para empezar, generemos aleatoriamente varios modelos lineales para ver qué pinta tienen. Para eso, podemos usar geom_abline () que toma una pendiente e intercepto como parámetros. models &lt;- tibble( a1 = runif(250, -20, 40), a2 = runif(250, -5, 5) ) ggplot(sim1, aes(x, y)) + geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) + geom_point() A simple vista podemos apreciar que algunos modelos son mejores que otros. Pero necesitamos una forma de cuantificar cuales son los mejores modelos. 8.1.2.2 distancias Una forma de definir mejor es pensar en aquel modelo que minimiza la distancia vertical con cada punto: Para eso, eligamos un modelo cualquiera: \\[ y= 7 + 1.5*x\\] (para que se vean mejor las distancias, corremos un poquito cada punto sobre el eje x) dist1 &lt;- sim1 %&gt;% mutate( dodge = rep(c(-1, 0, 1) / 20, 10), x1 = x + dodge, pred = 7 + x1 * 1.5 ) ggplot(dist1, aes(x1, y)) + geom_abline(intercept = 7, slope = 1.5, colour = &quot;grey40&quot;) + geom_point(colour = &quot;grey40&quot;) + geom_linerange(aes(ymin = y, ymax = pred), colour = &quot;#3366FF&quot;) La distancia de cada punto a la recta es la diferencia entre lo que predice nuestro modelo y el valor real Para computar la distancia, primero necesitamos una función que represente a nuestro modelo: Para eso, vamos a crear una función que reciba un vector con los parámetros del modelo, y el set de datos, y genere la predicción: model1 &lt;- function(a, data) { a[1] + data$x * a[2] } model1(c(7, 1.5), sim1) ## [1] 8.5 8.5 8.5 10.0 10.0 10.0 11.5 11.5 11.5 13.0 13.0 13.0 14.5 14.5 ## [15] 14.5 16.0 16.0 16.0 17.5 17.5 17.5 19.0 19.0 19.0 20.5 20.5 20.5 22.0 ## [29] 22.0 22.0 Ahora, necesitamos una forma de calcular los residuos y agruparlos. Esto lo vamos a hacer con el error cuadrático medio \\[ECM = \\sqrt\\frac{\\sum_i^n{(\\hat{y_i} - y_i)^2}}{n}\\] measure_distance &lt;- function(mod, data) { diff &lt;- data$y - model1(mod, data) sqrt(mean(diff ^ 2)) } measure_distance(c(7, 1.5), sim1) ## [1] 2.665212 8.1.2.3 Evaluando los modelos aleatorios Ahora podemos calcular el ECM para todos los modelos del dataframe models. Para eso utilizamos el paquete purrr, para ejecutar varias veces la misma función sobre varios elementos. Tenemos que pasar los valores de a1 y a2 (dos parámetros –&gt; map2), pero como nuestra función toma sólo uno (el vector a), nos armamos una función de ayuda para wrapear a1 y a2 sim1_dist &lt;- function(a1, a2) { measure_distance(c(a1, a2), sim1) } models &lt;- models %&gt;% mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist)) models ## # A tibble: 250 x 3 ## a1 a2 dist ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -14.0 4.46 8.75 ## 2 5.28 -1.89 23.6 ## 3 19.5 0.991 10.1 ## 4 -2.28 3.90 6.80 ## 5 -7.64 1.07 17.6 ## 6 -19.9 -0.594 39.4 ## 7 -13.5 0.601 26.2 ## 8 -7.11 4.39 7.23 ## 9 -13.1 3.35 11.1 ## 10 21.2 -2.13 13.6 ## # … with 240 more rows A continuación, superpongamos los 10 mejores modelos a los datos. Coloreamos los modelos por -dist: esta es una manera fácil de asegurarse de que los mejores modelos (es decir, los que tienen la menor distancia) obtengan los colores más brillantes. ggplot(sim1, aes(x, y)) + geom_point(size = 2, colour = &quot;grey30&quot;) + geom_abline( aes(intercept = a1, slope = a2, colour = -dist), data = filter(models, rank(dist) &lt;= 10) ) También podemos pensar en estos modelos como observaciones y visualizar con un gráfico de dispersión de a1 vsa2, nuevamente coloreado por -dist. Ya no podemos ver directamente cómo se compara el modelo con los datos, pero podemos ver muchos modelos a la vez. Nuevamente, destacamos los 10 mejores modelos, esta vez dibujando círculos rojos debajo de ellos. ggplot(models, aes(a1, a2)) + geom_point(data = filter(models, rank(dist) &lt;= 10), size = 4, colour = &quot;red&quot;) + geom_point(aes(colour = -dist)) 8.1.2.4 Grid search En lugar de probar muchos modelos aleatorios, podríamos ser más sistemáticos y generar una cuadrícula de puntos uniformemente espaciada (esto se denomina grid search). Elegimos los parámetros de la grilla aproximadamente mirando dónde estaban los mejores modelos en el gráfico anterior. grid &lt;- expand.grid( a1 = seq(-5, 20, length = 25), a2 = seq(1, 3, length = 25) ) %&gt;% mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist)) grid %&gt;% ggplot(aes(a1, a2)) + geom_point(data = filter(grid, rank(dist) &lt;= 10), size = 4, colour = &quot;red&quot;) + geom_point(aes(colour = -dist)) Cuando superponemos los 10 mejores modelos en los datos originales, todos se ven bastante bien: ggplot(sim1, aes(x, y)) + geom_point(size = 2, colour = &quot;grey30&quot;) + geom_abline( aes(intercept = a1, slope = a2, colour = -dist), data = filter(grid, rank(dist) &lt;= 10) ) 8.1.2.5 óptimo por métodos numéricos Podríamos imaginar este proceso iterativamente haciendo la cuadrícula más fina y más fina hasta que nos centramos en el mejor modelo. Pero hay una forma mejor de abordar ese problema: una herramienta de minimización numérica llamada búsqueda de Newton-Raphson. La intuición de Newton-Raphson es bastante simple: Se elige un punto de partida y se busca la pendiente más inclinada. Luego, desciende por esa pendiente un poco, y se repite una y otra vez, hasta que no se puede seguir bajando. En R, podemos hacer eso con optim (): necesitamos pasarle un vector de puntos iniciales. Elegimos 4 y 2, porque los mejores modelos andan cerca de esos valores le pasamos nuestra función de distancia, y los parámetros que nuestra función necesita (data) best &lt;- optim(c(4,2), measure_distance, data = sim1) best ## $par ## [1] 4.221029 2.051528 ## ## $value ## [1] 2.128181 ## ## $counts ## function gradient ## 49 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL ggplot(sim1, aes(x, y)) + geom_point(size = 2, colour = &quot;grey30&quot;) + geom_abline(intercept = best$par[1], slope = best$par[2]) 8.1.2.6 Óptimo para el modelo lineal Este procedimiento es válido para muchas familias de modelos. Pero para el caso del modelo lineal, conocemos otras formas de resolverlo Si nuestro modelo es \\[ y = a_1 + a_2x + \\epsilon \\] La solución del óptima que surge de minimizar el Error Cuadrático Medio es: \\[ \\hat{a_1} = \\bar{y} - \\hat{a_2}\\bar{x} \\] \\[ \\hat{a_2} = \\frac{\\sum_i^n (y_i -\\bar{y})(x_i -\\bar{x})}{\\sum_i^n (x_i- \\bar{x})} \\] R tiene una función específica para el modelo lineal lm(). Cómo esta función sirve tanto para regresiones lineales simples como múltiples, debemos especificar el modelo en las formulas: y ~ x sim1_mod &lt;- lm(y ~ x, data = sim1) 8.1.2.7 Interpretando la salida de la regresión summary(sim1_mod) ## ## Call: ## lm(formula = y ~ x, data = sim1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.1469 -1.5197 0.1331 1.4670 4.6516 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.2208 0.8688 4.858 4.09e-05 *** ## x 2.0515 0.1400 14.651 1.17e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.203 on 28 degrees of freedom ## Multiple R-squared: 0.8846, Adjusted R-squared: 0.8805 ## F-statistic: 214.7 on 1 and 28 DF, p-value: 1.173e-14 Analicemos los elementos de la salida: Residuals: La distribución de los residuos. Hablaremos más adelante. Coefficients: Los coeficientes del modelo. El intercepto y la variable explicativa Estimate: Es el valor estimado para cada parámetro Pr(&gt;|t|): Es el p-valor asociado al test que mide que el parámetro sea mayor que 0. Si el p-valor es cercano a 0, entonces el parámetro es significativamente mayor a 0. Multiple R-squared: El \\(R^2\\) indica que proporción del movimiento en \\(y\\) es explicado por \\(x\\). F-statistic: Es el resultado de un test de significatividad global del modelo. Con un p-valor bajo, rechazamos la hipótesis nula, que indica que el modelo no explicaría bien al fenómeno. interpretación de los parámetros: El valor estimado del parámetro se puede leer como “cuanto varía \\(y\\) cuando \\(x\\) varía en una unidad”. Es decir, es la pendiente de la recta 8.1.2.8 Análisis de los residuos Los residuos del modelo indican cuanto le erra el modelo en cada una de las observaciones. Es la distancia que intentamos minimizar de forma agregada. Podemos agregar los residuos al dataframe con add_residuals () de la librería modelr. sim1 &lt;- sim1 %&gt;% add_residuals(sim1_mod) sim1 %&gt;% sample_n(10) ## # A tibble: 10 x 3 ## x y resid ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2.13 -4.15 ## 2 5 16.0 1.55 ## 3 6 16.0 -0.574 ## 4 7 20.1 1.50 ## 5 3 7.36 -3.02 ## 6 4 14.3 1.83 ## 7 5 19.1 4.65 ## 8 5 11.7 -2.74 ## 9 7 19.9 1.35 ## 10 2 8.99 0.665 Si cuando miramos los residuos notamos que tienen una estructura, eso significa que nuestro modelo no esta bien especificado. En otros términos, nos olvidamos de un elemento importante para explicar el fenómeno. Lo que debemos buscar es que los residuos estén homogéneamente distribuidos en torno al 0. Hay muchas maneras de analizar los residuos. Una es con las estadísticas de resumen que muestra el summary. Otra forma es graficándolos. ggplot(sim1, aes(x, resid)) + geom_ref_line(h = 0, size = 2,colour = &quot;firebrick&quot;) + geom_point() 8.1.3 Regresión lineal múltiple Si bien escapa a los alcances de esta clase ver en detalle el modelo lineal múltiple, podemos ver alguna intuición. Notemos que el modelo ya no es una linea en un plano, sino que ahora el modelo es un plano, en un espacio de 3 dimensiones: Para cada par de puntos en \\(x_1\\) y \\(x_2\\) vamos a definir un valor para \\(y\\) El criterio para elegir el mejor modelo va a seguir siendo minimizar las distancias verticales. Esto quiere decir, respecto de la variable que queremos predecir. interpretación de los parámetros: El valor estimado del parámetro se puede leer como “cuanto varía \\(y\\) cuando \\(x\\) varía en una unidad, cuando todo lo demás permanece constante”. Noten que ahora para interpretar los resultados tenemos que hacer la abstracción de dejar todas las demás variables constantes Adjusted R-squared: Es similar a \\(R^2\\), pero ajusta por la cantidad de variables del modelo (nosotros estamos utilizando un modelo de una sola variable), sirve para comparar entre modelos de distinta cantidad de variables. 8.1.4 Para profundizar Estas notas de clase estan fuertemente inspiradas en los siguientes libros/notas: R para Cienca de Datos Apuntes regresión lineal Un punto pendiente de estas clases que es muy importante son los supuestos que tiene detrás el modelo lineal. "],
["practica-guiada-6.html", "8.2 Práctica Guiada", " 8.2 Práctica Guiada library(tidyverse) 8.2.1 Datos de Properati Para este ejercicio utilizaremos los datos provistos por Properati: https://www.properati.com.ar/data/ Primero acondicionamos la base original, para quedarnos con una base más fácil de trabajar, y que contiene unicamente los datos interesantes. (no es necesario correrlo) ar_properties &lt;- read_csv(&quot;~/Downloads/ar_properties.csv&quot;) ar_properties %&gt;% filter(operation_type==&#39;Venta&#39;, property_type %in% c(&#39;Casa&#39;,&#39;PH&#39;,&#39;Departamento&#39;), currency==&#39;USD&#39;, l1==&#39;Argentina&#39;, l2==&#39;Capital Federal&#39;, !is.na(rooms), !is.na(surface_total), !is.na(surface_covered), !is.na(bathrooms), !is.na(l3)) %&gt;% select(-c(lat,lon, title,description, ad_type,start_date, end_date,operation_type,currency, l1, l2,l4,l5,l6,price_period,bedrooms)) %&gt;% saveRDS(&#39;../fuentes/datos_properati.RDS&#39;) df &lt;- read_rds(&#39;../fuentes/datos_properati.RDS&#39;) glimpse(df) ## Observations: 52,246 ## Variables: 9 ## $ id &lt;chr&gt; &quot;OgLe3YSDR0da+JUQZgmTtA==&quot;, &quot;Z3j1BtQN1kzuJr20com… ## $ created_on &lt;date&gt; 2019-05-09, 2019-05-09, 2019-05-09, 2019-05-09,… ## $ l3 &lt;chr&gt; &quot;Nuñez&quot;, &quot;Nuñez&quot;, &quot;Almagro&quot;, &quot;Belgrano&quot;, &quot;Flores… ## $ rooms &lt;dbl&gt; 3, 3, 3, 5, 5, 3, 3, 2, 5, 5, 4, 2, 3, 5, 3, 3, … ## $ bathrooms &lt;dbl&gt; 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 4, 2, 3, … ## $ surface_total &lt;dbl&gt; 77, 97, 69, 230, 168, 65, 95, 50, 181, 180, 89, … ## $ surface_covered &lt;dbl&gt; 68, 65, 69, 200, 168, 65, 92, 38, 110, 120, 118,… ## $ price &lt;dbl&gt; 180000, 265000, 230000, 380000, 255000, 119000, … ## $ property_type &lt;chr&gt; &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, … summary(df$price) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 6000 119000 170000 251944 272000 6000000 df[df$price&lt;10000,] ## # A tibble: 4 x 9 ## id created_on l3 rooms bathrooms surface_total surface_covered ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 uZe6… 2019-03-28 Pale… 5 4 340 320 ## 2 +JnI… 2019-04-01 Parq… 1 1 31 31 ## 3 MEQM… 2019-03-15 Puer… 3 3 275 220 ## 4 o6Qf… 2019-04-30 Reco… 3 2 340 200 ## # … with 2 more variables: price &lt;dbl&gt;, property_type &lt;chr&gt; df &lt;- df %&gt;% filter(price&gt;10000) Tenemos un par de outliers que no tienen mucho sentido. Es posible que el precio este mal cargado. df[df$price&gt;5000000,] ## # A tibble: 11 x 9 ## id created_on l3 rooms bathrooms surface_total surface_covered ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Z0NE… 2019-04-13 Reco… 6 3 600 600 ## 2 gRZz… 2019-01-25 Reco… 6 3 600 600 ## 3 sP/J… 2019-05-18 Reco… 8 5 677 568 ## 4 VVkm… 2019-04-05 Reco… 10 3 978 489 ## 5 h6gp… 2019-06-15 Reco… 6 3 600 600 ## 6 HWNt… 2019-06-19 San … 3 1 60 56 ## 7 e2Wf… 2019-01-28 Pale… 4 4 404 404 ## 8 OzkE… 2019-01-28 Pale… 4 4 404 404 ## 9 6DhC… 2019-02-01 Caba… 1 1 41 37 ## 10 Jz4a… 2019-03-01 Caba… 1 1 41 37 ## 11 1R9Q… 2019-01-17 Caba… 1 1 41 37 ## # … with 2 more variables: price &lt;dbl&gt;, property_type &lt;chr&gt; Los precios más alto tienen algunas cosas sorprendentes, pero sería arriesgado descartarlos por errores. lm_fit &lt;- lm(price~ l3+ rooms + bathrooms + surface_total + property_type,data = df) summary(lm_fit) ## ## Call: ## lm(formula = price ~ l3 + rooms + bathrooms + surface_total + ## property_type, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2152714 -72322 -4147 46114 5284489 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.599e+05 1.388e+04 -11.520 &lt; 2e-16 *** ## l3Agronomía 2.024e+04 2.605e+04 0.777 0.437102 ## l3Almagro -7.962e+03 1.295e+04 -0.615 0.538764 ## l3Balvanera -2.616e+04 1.360e+04 -1.924 0.054387 . ## l3Barracas 5.459e+02 1.591e+04 0.034 0.972629 ## l3Barrio Norte 6.416e+04 1.331e+04 4.821 1.43e-06 *** ## l3Belgrano 1.212e+05 1.290e+04 9.396 &lt; 2e-16 *** ## l3Boca -4.934e+04 2.020e+04 -2.443 0.014559 * ## l3Boedo -1.421e+04 1.553e+04 -0.915 0.360345 ## l3Caballito 6.359e+03 1.298e+04 0.490 0.624125 ## l3Catalinas -2.566e+04 1.059e+05 -0.242 0.808533 ## l3Centro / Microcentro -3.333e+04 1.969e+04 -1.693 0.090542 . ## l3Chacarita 2.843e+04 1.609e+04 1.768 0.077139 . ## l3Coghlan 5.894e+04 1.643e+04 3.587 0.000335 *** ## l3Colegiales 3.945e+04 1.455e+04 2.710 0.006724 ** ## l3Congreso -2.853e+04 1.610e+04 -1.773 0.076275 . ## l3Constitución -2.953e+04 1.767e+04 -1.671 0.094633 . ## l3Flores -2.403e+04 1.363e+04 -1.763 0.077967 . ## l3Floresta -1.220e+04 1.516e+04 -0.804 0.421184 ## l3Las Cañitas 1.193e+05 1.758e+04 6.785 1.17e-11 *** ## l3Liniers -2.029e+04 1.592e+04 -1.275 0.202348 ## l3Mataderos -3.332e+04 1.612e+04 -2.067 0.038736 * ## l3Monserrat -9.560e+03 1.570e+04 -0.609 0.542461 ## l3Monte Castro 1.875e+04 1.793e+04 1.046 0.295781 ## l3Nuñez 9.191e+04 1.373e+04 6.695 2.18e-11 *** ## l3Once -2.203e+04 1.598e+04 -1.379 0.168006 ## l3Palermo 1.276e+05 1.272e+04 10.033 &lt; 2e-16 *** ## l3Parque Avellaneda -1.666e+04 2.199e+04 -0.758 0.448651 ## l3Parque Centenario -3.832e+04 1.523e+04 -2.515 0.011903 * ## l3Parque Chacabuco -1.329e+03 1.569e+04 -0.085 0.932517 ## l3Parque Chas 2.209e+04 2.267e+04 0.975 0.329726 ## l3Parque Patricios -1.126e+04 1.768e+04 -0.637 0.524163 ## l3Paternal -2.778e+03 1.550e+04 -0.179 0.857733 ## l3Pompeya -6.158e+04 2.211e+04 -2.786 0.005340 ** ## l3Puerto Madero 5.295e+05 1.457e+04 36.353 &lt; 2e-16 *** ## l3Recoleta 1.294e+05 1.309e+04 9.883 &lt; 2e-16 *** ## l3Retiro 7.507e+04 1.571e+04 4.779 1.76e-06 *** ## l3Saavedra 3.674e+04 1.485e+04 2.473 0.013387 * ## l3San Cristobal -1.197e+04 1.479e+04 -0.809 0.418323 ## l3San Nicolás -4.616e+03 1.534e+04 -0.301 0.763503 ## l3San Telmo 1.763e+04 1.460e+04 1.208 0.227176 ## l3Tribunales -4.234e+04 2.555e+04 -1.657 0.097553 . ## l3Velez Sarsfield 1.664e+03 2.487e+04 0.067 0.946644 ## l3Versalles 4.516e+03 1.988e+04 0.227 0.820295 ## l3Villa Crespo 1.072e+04 1.303e+04 0.823 0.410681 ## l3Villa del Parque 2.440e+04 1.470e+04 1.660 0.096951 . ## l3Villa Devoto 3.089e+04 1.440e+04 2.146 0.031896 * ## l3Villa General Mitre -2.567e+04 2.024e+04 -1.268 0.204656 ## l3Villa Lugano -1.002e+05 1.749e+04 -5.729 1.02e-08 *** ## l3Villa Luro 7.208e+03 1.617e+04 0.446 0.655849 ## l3Villa Ortuzar 2.826e+04 2.042e+04 1.383 0.166525 ## l3Villa Pueyrredón 2.686e+04 1.590e+04 1.689 0.091191 . ## l3Villa Real 1.343e+04 2.592e+04 0.518 0.604258 ## l3Villa Riachuelo -5.135e+04 4.988e+04 -1.029 0.303274 ## l3Villa Santa Rita 6.264e+03 1.909e+04 0.328 0.742874 ## l3Villa Soldati -9.211e+04 3.636e+04 -2.534 0.011295 * ## l3Villa Urquiza 4.076e+04 1.333e+04 3.058 0.002230 ** ## rooms 5.199e+04 8.989e+02 57.839 &lt; 2e-16 *** ## bathrooms 1.419e+05 1.461e+03 97.128 &lt; 2e-16 *** ## surface_total 5.808e+00 1.141e+00 5.092 3.56e-07 *** ## property_typeDepartamento 4.855e+03 5.267e+03 0.922 0.356609 ## property_typePH -4.780e+04 5.691e+03 -8.399 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 210300 on 52180 degrees of freedom ## Multiple R-squared: 0.4872, Adjusted R-squared: 0.4866 ## F-statistic: 812.7 on 61 and 52180 DF, p-value: &lt; 2.2e-16 ¿ Qué pasó con las variables no numéricas? ¿Son significativos los estimadores? ¿cuales? ¿Cómo se leen los valores de los estimadores? Dado que muchos de los barrios no explican significativamente los cambios en los precios, no esta bueno conservarlos todos. A su vez, no sabemos respecto a qué barrio se compara. Una solución puede ser agrupar los barrios en tres categorías respecto a su efecto en el precio: Alto Medio Bajo En particular, podemos notar de esta primera regresión que algunos barrios tienen un efecto significativo en subir el valor de la propiedad, como Belgrano o Recoleta. Para construir la nueva variable, podemos ver el precio promedio del metro cuadrado por barrio df_barrios &lt;- df %&gt;% group_by(l3) %&gt;% summarise(precio_m2 = mean(price/surface_total)) ggplot(df_barrios,aes(precio_m2)) + geom_histogram() summary(df_barrios$precio_m2) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 871.2 2031.8 2147.3 2346.0 2560.0 6068.5 Con este gráfico vemos que que hay muchos barrios con un precio promedio cercano a 2500 dólares el \\(m^2\\). Podemos dividr los tres grupos al rededor de los quartiles 1 y 3. &lt;2000 bajo 2000-2500 medio 2500 alto df_barrios &lt;- df_barrios %&gt;% mutate(barrio= case_when(precio_m2&lt;2000 ~ &#39;bajo&#39;, precio_m2&gt;2000 &amp; precio_m2&lt;2500 ~ &#39;medio&#39;, precio_m2&gt;2500 ~ &#39;alto&#39;)) df_barrios %&gt;% sample_n(10) ## # A tibble: 10 x 3 ## l3 precio_m2 barrio ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Almagro 2457. medio ## 2 Barrio Norte 3221. alto ## 3 Abasto 2635. alto ## 4 Villa Luro 2147. medio ## 5 San Nicolás 2439. medio ## 6 Parque Patricios 1925. bajo ## 7 Las Cañitas 3724. alto ## 8 Flores 2108. medio ## 9 Constitución 1758. bajo ## 10 Parque Chas 2305. medio Con esta nueva variable podemos modificar la tabla original. df &lt;- df %&gt;% left_join(df_barrios, by=&#39;l3&#39;) y volvemos a calcular el modelo lm_fit &lt;- lm(price~ barrio+ rooms + bathrooms + surface_total + property_type,data = df) summary(lm_fit) ## ## Call: ## lm(formula = price ~ barrio + rooms + bathrooms + surface_total + ## property_type, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2145645 -71277 -11187 42472 5307946 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.041e+05 6.349e+03 -16.396 &lt; 2e-16 *** ## barriobajo -1.097e+05 4.231e+03 -25.939 &lt; 2e-16 *** ## barriomedio -9.342e+04 2.150e+03 -43.445 &lt; 2e-16 *** ## rooms 4.808e+04 9.293e+02 51.732 &lt; 2e-16 *** ## bathrooms 1.602e+05 1.499e+03 106.867 &lt; 2e-16 *** ## surface_total 5.485e+00 1.198e+00 4.580 4.67e-06 *** ## property_typeDepartamento 2.370e+04 5.352e+03 4.428 9.51e-06 *** ## property_typePH -3.906e+04 5.920e+03 -6.598 4.22e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 220800 on 52234 degrees of freedom ## Multiple R-squared: 0.4338, Adjusted R-squared: 0.4338 ## F-statistic: 5718 on 7 and 52234 DF, p-value: &lt; 2.2e-16 Si queremos que compare contra ‘barrio medio’ podemos convertir la variable en factor y explicitar los niveles df &lt;- df %&gt;% mutate(barrio = factor(barrio, levels = c(&#39;medio&#39;, &#39;alto&#39;,&#39;bajo&#39;))) lm_fit &lt;- lm(price~ barrio+ rooms + bathrooms + surface_total + property_type,data = df) summary(lm_fit) ## ## Call: ## lm(formula = price ~ barrio + rooms + bathrooms + surface_total + ## property_type, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2145645 -71277 -11187 42472 5307946 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.975e+05 6.215e+03 -31.783 &lt; 2e-16 *** ## barrioalto 9.342e+04 2.150e+03 43.445 &lt; 2e-16 *** ## barriobajo -1.632e+04 4.321e+03 -3.777 0.000159 *** ## rooms 4.808e+04 9.293e+02 51.732 &lt; 2e-16 *** ## bathrooms 1.602e+05 1.499e+03 106.867 &lt; 2e-16 *** ## surface_total 5.485e+00 1.198e+00 4.580 4.67e-06 *** ## property_typeDepartamento 2.370e+04 5.352e+03 4.428 9.51e-06 *** ## property_typePH -3.906e+04 5.920e+03 -6.598 4.22e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 220800 on 52234 degrees of freedom ## Multiple R-squared: 0.4338, Adjusted R-squared: 0.4338 ## F-statistic: 5718 on 7 and 52234 DF, p-value: &lt; 2.2e-16 8.2.1.1 Feature engineering. Lo que hicimos arriba con los barrios se conoce como feature engineerin: Generamos una nueva variable a partir de las anteriores para mejorar nuestro modelo. ¿Qué otras modificaciones podemos hacer? Hay una que ya hicimos: En lugar de pensar en el precio total, podemos pensar en el precio por \\(m^2\\). De esta manera ya no tendría sentido agregar la variable surface_total lm_fit &lt;- lm(precio_m2 ~ barrio + rooms + bathrooms + property_type,data = df) summary(lm_fit) ## ## Call: ## lm(formula = precio_m2 ~ barrio + rooms + bathrooms + property_type, ## data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2071.97 -241.41 55.51 214.05 2993.52 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1935.419 13.106 147.670 &lt; 2e-16 *** ## barrioalto 892.491 4.535 196.790 &lt; 2e-16 *** ## barriobajo -461.046 9.112 -50.595 &lt; 2e-16 *** ## rooms -22.684 1.959 -11.579 &lt; 2e-16 *** ## bathrooms 133.009 3.161 42.084 &lt; 2e-16 *** ## property_typeDepartamento 227.481 11.285 20.158 &lt; 2e-16 *** ## property_typePH 99.545 12.485 7.973 1.58e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 465.7 on 52235 degrees of freedom ## Multiple R-squared: 0.552, Adjusted R-squared: 0.5519 ## F-statistic: 1.073e+04 on 6 and 52235 DF, p-value: &lt; 2.2e-16 que pasó con rooms? Al normalizar el precio por los metros, rooms pasa de tomar valores positivos a negativos. Eso significa que rooms estaba correlacionado con el tamaño, y por lo tanto cuantos más cuartos, mayor el valor. Al normalizar podemos ver que, dado un metraje, más cuartos reducen el precio: Preferimos ambientes más grandes tal vez? predecir Para predecir un nuevo caso, podemos construir un dataframe con las variables. Por ejemplo caso_nuevo &lt;- tibble(barrio=&#39;alto&#39;, rooms=3, bathrooms=2, property_type=&#39;Departamento&#39;, surface_total=78) predict(lm_fit,newdata = caso_nuevo) ## 1 ## 3253.356 Pero debemos recordar que este es el valor por metro cuadrado. Para obtener lo que realmente nos interesa, tenemos que hacer el camino inverso del feature engenieering: predict(lm_fit,caso_nuevo)*caso_nuevo$surface_total ## 1 ## 253761.8 8.2.1.2 Para seguir practicando Un problema de lo que vimos en esta práctica es que las salidas de summary(lm_fit) es una impresión en la consola. Es muy difícil seguir trabajando con esos resultados. Para resolver esto hay un par de librerías que incorporan el modelado lineal al flujo del tidyverse: Broom Modelr "],
["references.html", "References", " References "]
]
