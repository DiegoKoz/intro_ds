---
title: Modelo Lineal
output:
  html_notebook:
    toc: yes
    toc_float: yes
date: ""
subtitle: Práctica Guiada
---


```{r message=FALSE, warning=FALSE}
library(tidyverse)
```


Para este ejercicio utilizaremos los datos provistos por Properati: https://www.properati.com.ar/data/

(Primero acondicionamos la base original, para quedarnos con una base más fácil de trabajar, y que contiene unicamente los datos interesantes. También eliminamos previamente los outliers)

```{r}
df <- read_rds('../fuentes/datos_properati.RDS')

df %>% sample_n(10)
```

### Primer modelo

```{r}
lm_fit <- lm(price~ l3+ rooms + bathrooms + surface_total + property_type,data = df)
```

```{r}
summary(lm_fit)
```

> ¿ Qué pasó con las variables no numéricas?
> ¿Son significativos los estimadores? ¿cuales?
> ¿Cómo se leen los valores de los estimadores?


$$
\Delta y = \beta_1 \Delta x
$$

### Feature engineering.

Se denomina ingeniería de features a la construcción de nuevas variables a partir de las originales. 

Por ejemplo, dado que muchos de los barrios no explican significativamente los cambios en los precios, no esta bueno conservarlos todos. A su vez, no sabemos respecto a qué barrio se compara.

Una solución puede ser agrupar los barrios en tres categorías respecto a su efecto en el precio: 

- Alto
- Medio
- Bajo

En particular, podemos notar de esta primera regresión que algunos barrios tienen un efecto significativo en subir el valor de la propiedad, como Belgrano o Recoleta. 

Para construir la nueva variable, podemos ver el precio promedio del metro cuadrado por barrio

```{r}

df_barrios <- df %>% 
  group_by(l3) %>% 
  summarise(precio_m2 = mean(price/surface_total)) 
  
ggplot(df_barrios,aes(precio_m2)) +
  geom_dotplot(method = 'histodot', binwidth = 100)


summary(df_barrios$precio_m2)
```

Con este gráfico vemos que que hay muchos barrios con un precio promedio cercano a 2500 dólares el $m^2$.

Podemos dividr los tres grupos al rededor de los quartiles 1 y 3. 

- <2000 bajo
- 2000-2500 medio
- \>2500 alto

```{r}
df_barrios <- df_barrios %>% 
  mutate(barrio= case_when(precio_m2<=2100 ~ 'bajo',
                           precio_m2>2100 & precio_m2<2400 ~ 'medio',
                          #between(precio_m2,2100,2400)~'medio', # Otra forma de escribirlo
                           precio_m2>=2400 ~ 'alto'))

df_barrios %>% 
  sample_n(10)
```

Con esta nueva variable podemos modificar la tabla original.

```{r}
df <- df %>% 
  left_join(df_barrios, by='l3')
```


y volvemos a calcular el modelo



```{r}
lm_fit <- lm(price~ barrio+ rooms + bathrooms + surface_total + property_type,data = df)

summary(lm_fit)
```

Si queremos que compare contra 'barrio medio' podemos convertir la variable en factor y explicitar los niveles

```{r}
df <- df %>% 
  mutate(barrio = factor(barrio, levels = c('medio', 'alto','bajo')))

lm_fit <- lm(price~ barrio+ rooms + bathrooms + surface_total + property_type,data = df)

summary(lm_fit)
```


> Cuál es el mejor modelo? Con los barrios colapsados en tres niveles, o con todos los barrios?


### Transformaciones log. 


```{r}
df %>% 
  sample_n(10000) %>% 
ggplot(., aes(surface_total, price))+
  geom_point(alpha=0.75)+
  geom_smooth(method = 'lm')
```

Nuestros datos no tienen varianza constante, tienen _heterocedasticidad_. Que la varianza sea constante es uno de los supestos del modelo lineal.

```{r}
df %>% 
  sample_n(10000) %>% 
ggplot(., aes(surface_total, price))+
  geom_point(alpha=0.75)+
  geom_smooth(method = 'lm')+
  scale_y_log10()+
  scale_x_log10()
```

La transformación logarítmica parece ser una buena idea para nuestros datos


```{r}
lm_fit <- lm(log(price)~ barrio+ rooms + bathrooms + log(surface_total) + property_type,data = df)

summary(lm_fit)
```

- El $R^2$ pasó de 0.6977 a 0.7833!!

- Cómo se interpretan ahora los nuevos parámetros?


| Modelo      | Variable Dependiente | Variable Independiente | Interpretación $\beta_1$      |
|-------------|------------------|--------------------|---------------------------------------|
| Nivel-Nivel | $y$              | $x$                | $\Delta y = \beta_1 \Delta x$         |
| Nivel-log   | $y$              | $log(x)$           | $\Delta y = (\beta_1/100)\% \Delta x$ |
| Log-nivel   | $log(y)$         | $x$                | $\% \Delta y = (100 \beta_1) \Delta x$|
| Log-log     | $log(y)$         | $log(x)$           | $\% \Delta y = \beta_1 \% \Delta x$   |

### Predicciones

Para predecir un nuevo caso, podemos construir un dataframe con las variables. Por ejemplo
```{r}
caso_nuevo <- tibble(barrio='alto',
       rooms=2,
       bathrooms=2,
       property_type='Departamento',
       surface_total=78) 

predict(lm_fit,newdata = caso_nuevo)
```




Pero debemos recordar que este es el valor del logarítmo del percio. Por lo tanto tenemos que realizar el caminio inverso con la función `exp`


```{r}
exp(predict(lm_fit,caso_nuevo))
```


### Para seguir practicando

Un problema de lo que vimos en esta práctica es que las salidas de `summary(lm_fit)` es una impresión en la consola. Es muy difícil seguir trabajando con esos resultados. Para resolver esto hay un par de librerías que incorporan el modelado lineal al flujo del tidyverse:

  + [Broom](https://cran.r-project.org/web/packages/broom/vignettes/broom.html)
  + [Modelr](https://modelr.tidyverse.org/)

