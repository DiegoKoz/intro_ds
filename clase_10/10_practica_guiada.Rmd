---
title:  Minería de Textos
output:
  html_notebook:
    toc: yes
    toc_float: yes
date: ""
subtitle: Práctica Guiada
---



```{r message=FALSE, warning=FALSE}
# install.packages("rtweet")
library(rtweet)
library(tidyverse)
library(tm)
library(wordcloud2)
library(topicmodels)
library(LDAvis)
library(tsne)
```

### Descargas de tweets con `rtweet`


```{r eval=FALSE}
rt <- search_tweets(q = "metrovias OR bondi OR Subte OR autopista OR transporte público",type = "mixed", n = 18000, include_rts = FALSE,
                    lang='es')

saveRDS(rt,'../fuentes/rt.RDS')
```

```{r}
rt <- read_rds('../fuentes/rt.RDS')
```


```{r}
rt %>% 
  sample_n(10)
```

```{r}
range(rt$created_at)
```

Nos da los tweets de los últimos nueve días, o el máximo que indicamos más reciente

```{r}
rt %>%
  ts_plot("3 hours") +
  ggplot2::theme_minimal() +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "Frecuencia de los tweets relacionados al tránsito",
    subtitle = "Agregado a intervalos de tres horas")
```

- Me quedo con el texto

```{r}
texto <-  rt$text 

texto[1:10]
```

Este texto es necesario limpiarlo para que sea más fácil de utilizar.

### Armado del corpus con `tm`

- Primero creo un objeto de tipo Corpus. Utilizamos algo distinto a los conocidos vectores of dataframes porque es un objeto optimizado para trabajar con texto. Esto nos permite que los procesos sean mucho más eficientes, y por lo tanto trabajar con grandes corpus de manera rápida

```{r}
myCorpus = Corpus(VectorSource(texto))
```


### Limpieza del Corpus

- Con la función `tm_map` podemos iterar sobre el corpus aplicando una transformación sobre cada documento (se acuerdan de la librería `PURRR`?)

En este caso, para la limpieza utilizaremos las siguientes transformaciones.

1. Pasar todo a minúscula (cómo la función que usamos no es de la librería `tm` tenemos que usar también `content_transformer` )
2. Sacar la puntuación
3. Sacar los números
4. Sacar las stopwords
```{r warning=FALSE}
myCorpus = tm_map(myCorpus, content_transformer(tolower))
myCorpus = tm_map(myCorpus, removePunctuation)
myCorpus = tm_map(myCorpus, removeNumbers)
myCorpus = tm_map(myCorpus, removeWords, stopwords(kind = "es"))
```

También deberíamos sacar las palabras que utilizamos para descargar la información.

```{r}
# metrovias OR bondi OR Subte OR autopista OR transporte público
myCorpus = tm_map(myCorpus, removeWords, c('metrovias', 'bondi','subte','autopista','transporte', 'público' ))
```


```{r}
inspect(myCorpus[1:10])
```

podemos ver que nos quedaron unos __\n__ que son la forma de representar el "enter". Lo mejor sería eliminarlos.

También queremos sacar los links. Para eso vamos a usar expresiones regulares para definir el patron que tiene un link, y luego crearemos una función que los elimine.

#### Expresiones regulares

Para que sea más sencilla la construcción de la expresión regular, usamos la librería [RVerbalExpressions](https://rverbalexpressions.netlify.com/index.html)

```{r}
# devtools::install_github("VerbalExpressions/RVerbalExpressions")
library(RVerbalExpressions)

expresion <- rx() %>% 
  rx_find('http') %>% 
  rx_maybe('s') %>% 
  # rx_maybe('://') %>% #como ya lo pasamos por los otros filtros, ya no hay puntuacion
  rx_anything_but(value = ' ')

expresion

```

Probamos la expresion con un ejemplo
```{r}
txt <- "detienen  dos menores  asalto  transporte público\n\nhttpstcoffiewafx httpstcoerfywrlzy"      
str_remove_all(txt, pattern = expresion)
```

Lo pasamos por el corpus

```{r}
myCorpus = tm_map(myCorpus, content_transformer(function(x) str_remove_all(x, pattern = expresion)))
myCorpus = tm_map(myCorpus, content_transformer(function(x) str_remove_all(x, pattern = '\n')))

```

```{r}
inspect(myCorpus[1:10])
```


Creamos una matriz de Término-documento

```{r}
myDTM = DocumentTermMatrix(myCorpus, control = list(minWordLength = 1))
```



```{r}
inspect(myDTM)
```

```{r}
palabras_frecuentes <- findMostFreqTerms(myDTM,n = 25, INDEX = rep(1,nDocs(myDTM)))[[1]]

palabras_frecuentes
```


```{r}
palabras_frecuentes <- tibble(word = names(palabras_frecuentes), freq =palabras_frecuentes)

wordcloud2(palabras_frecuentes, shuffle = FALSE)
```

### Topic Modeling


necesito eliminar los documentos vacíos (que luego de la limpieza quedaron sin ningúna palabra)

```{r}
ui = unique(myDTM$i)
dtm = myDTM[ui,]

dim(myDTM)
dim(dtm)
```

```{r eval=FALSE}
lda_fit <- LDA(dtm, k = 10,method = "Gibbs", control = list(delta=0.6,seed = 1234))
lda_fit
saveRDS(lda_fit,'resultados/lda_fit.rds')
```


```{r}
lda_fit <- readRDS('../resultados/lda_fit.rds')
```


```{r}
Terms <- terms(lda_fit, 10)
Terms
```


```{r}
topicmodels_json_ldavis <- function(fitted, dtm){
    svd_tsne <- function(x) tsne(svd(x)$u)

    # Find required quantities
    phi <- as.matrix(posterior(fitted)$terms)
    theta <- as.matrix(posterior(fitted)$topics)
    vocab <- colnames(phi)
    term_freq <- slam::col_sums(dtm)

    # Convert to json
    json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
                            vocab = vocab,
                            mds.method = svd_tsne,
                            plot.opts = list(xlab="tsne", ylab=""),
                            doc.length = as.vector(table(dtm$i)),
                            term.frequency = term_freq)

    return(json_lda)
}
```

```{r}
json_res <- topicmodels_json_ldavis(lda_fit, dtm)

serVis(json_res)
```