---
title: Minería de Textos
subtitle: Explicación
date: ""
output:
  html_notebook: 
    toc: true
    toc_float: true 
---


![](img/wordcloud.png)

### Introducción

- Junto con las imágenes y los audios, los textos son una fuente de datos no estructurados que se multiplicó en los últimos años.

- Para poder hacer uso de la información que contienen es necesario procesar los documentos originales a un formato lo suficientemente estandarizado como para que pueda alimentar algún tipo de modelo

- En la clase de hoy veremos un repaso de algunas de las técnicas más usuales para normalizar la información de documentos.

### Bag of Words

- Los documentos se pueden caracterizar por las palabras que contienen. Esto esconde el supuesto fuerte de independencia. No estamos considerando el orden

- Para este tipo de técnicas, una buena representación de la información es una bolsa de palabras. Un formato que indica la cantidad de veces que aparece una palabra en un documento.

- También se conocen como Matrices Documento-Término o Término-Documento, según la orientación

__Ejemplo__: opiniones de trip advisor:


```{r}
library(tm)
doc1='Lugar espectacular e inolvidable'
doc2='Precioso lugar, la comida era espectacular, de 10, precioso!'

texto <- c(doc1,doc2)

myCorpus = VCorpus(VectorSource(texto))
myDTM = DocumentTermMatrix(myCorpus, control = list(minWordLength = 1))
m = as.matrix(myDTM)
m
```


![](img/DTM_bad.png)

- Nosotros sabemos que el significado de “lugar” y “lugar,” es el mismo

- Al no estar normalizada la información, la BoW genera matrices muy grandes y esparsas, que son poco útiles para trabajar


### Normalización


Para construir el Bag of Words se debe considerar los siguientes procesos:

- __Tokenization__: Es el proceso de partir un string de texto en palabras y signos de puntuación.
 
- __Eliminar puntuación__.

- __Stop Words__: remover las palabras más comunes del idioma (“el”, “la”, “los”, “de”) ya que aparecen en todos los documentos y no aportan información valiosa para distinguirlos.

- __Lemmatization__: Es la representación de todas las formas flexionadas (plural, femenino, conjugado, etc.). Para esto, es necesario contar con una base de datos léxica. Para esto podemos usar [koRpus](https://github.com/unDocUMeantIt/koRpus) que incluye el lexicón TreeTagger.

- __Stemming__: Es similar a la lematización, pero no se basa en las estructuras lexicales, sino que realiza una aproximación, quedándose con las primeras letras de la palabra. 

- _N-gramas_: A veces los conceptos que permiten distinguir entre documentos se componen de más de una palabra, por ejemplo:
  - “a duras penas” (trigrama),
  - “Buenos Aires” (bigrama) 

- Las expresiones idiomáticas o los nombres propios cambian radicalmente de sentido si se separan sus componentes. 
  - Imaginense si quisiéramos clasificar la posición política de izquierda a derecha de los “Nacional Socialistas”!

__Ejemplo__: Limpiando el texto:


```{r}

doc1='Lugar espectacular e inolvidable'
doc2='Precioso lugar, la comida era espectacular, de 10, precioso!'

texto <- c(doc1,doc2)

myCorpus = VCorpus(VectorSource(texto))
myCorpus = tm_map(myCorpus, content_transformer(tolower))
myCorpus = tm_map(myCorpus, removePunctuation)
myCorpus = tm_map(myCorpus, removeNumbers)
myCorpus = tm_map(myCorpus, removeWords, stopwords(kind = "es"))
myDTM = DocumentTermMatrix(myCorpus, control = list(minWordLength = 1))
m = as.matrix(myDTM)
m
```


### Distancia de palabras

La distancia de palabras se puede entender desde distintos lugares:


- __Distancia de caracteres__: Refiere a la similitud de escritura “Mueve” vs “Nueve”
- __Distancia conceptual__:  Refiere a la similitud del concepto: “Perro” vs “Labrador”



#### Distancia de caracteres

- __Distancia de Levenshtein__ o distancia de edición es el número mínimo de operaciones requeridas para transformar una cadena de caracteres en otra. Una operación puede ser una inserción, eliminación o sustitución de un carácter.

- __Jaro Winkler__: Esta medida de similitud da mejores puntajes a los strings que son similares en el principio de la oración. $0 < sim_{jw}<1$, donde 1 significa que las palabras son idénticas (excepto que p=0.25 y compartan los primeros 4 caracteres). y 0 significa que no se parecen en nada


#### Distancia Conceptual


-  __Word Embeddings__: Son una representación vectorial de las palabras que se construye a partir de observar una gran cantidad de documentos.

- __Word2Vec__ fue la primera implementación de esta idea. Se entrena una red neuronal para predecir el contexto de una palabra, y luego se utiliza una matriz que se construye dentro de la red como representación de las palabras.

![](img/skipgram.png)


__ejemplo__: [Word Embedding de textos marxistas](https://projector.tensorflow.org/?config=https%3A%2F%2Fraw.githubusercontent.com%2FDiegoKoz%2FMIA_text_mining%2Fmaster%2FWordVectors%2Ftensorboard_config.json)

### Distancia de Documentos

#### Similitud Coseno


- En el modelo de __BoW__ representamos a todos los documentos como vectores n-dimensionales que toman valores en el espacio de los números enteros. 

- La dimensión n del espacio está determinada por lo largo del vocabulario utilizado en el corpus. 

- Para comparar la similitud entre dos documentos, podemos utilizar la __similitud coseno__ entre sus representaciones vectoriales.  Intuitivamente, la similitud coseno es una medida de correlación de vectores que representan atributos en lugar de variables que se mueven en un espacio continuo. 



recordemos primero algunas definiciones. 


El producto interno entre dos vectores x,y se define como:


$$
\langle x,y \rangle=\sum_i x_i y_i = \|x\|\ \|y\|\cos(\theta)
$$


Por su parte, la norma 2 de un vector x se define como:

$$
||x||=\sqrt{\sum_i x^2_i}
$$

![](img/cosine_similarity.png)

$$
CosSim(x,y) = \frac{\langle x,y \rangle}{\|x\|\ \|y\|} = \frac{\sum_i x_iy_i}{\sum_i x^2_i\sum_i y^2_i} 
$$


- Cuantas más palabras compartan los documentos, mayor es el producto punto. Este a su vez se normaliza por el tamaño de cada documento. 

- Este valor va de 1 para los documentos son identicos a 0 cuando son totalmente distintos.



### Conclusiones

- Una forma de representar la información de documentos de texto es el Bag of Words, con distintas variantes.

- Existen muchas herramientas (lematización, tokenización, stop-words, N-gramas) para limpiar el texto original.  Estas no son universalmente válidas, y se deben utilizar a conciencia, dependiendo del modelo que alimentarán los datos, y el tipo de complejidades que presenta el dataset.

- También vimos otras herramientas que en ciertos casos pueden ayudar a corregir errores de un conjunto de datos que fue transcrito desde audio, como las distancias de levenshtein y Jaro Winkler



